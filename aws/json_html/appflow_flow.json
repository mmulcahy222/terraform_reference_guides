{"resource-aws-appflow-flow": "<h1 id=\"resource-aws_appflow_flow\">Resource: aws_appflow_flow</h1><p>Provides an AppFlow flow resource.</p>", "example-usage": "<h2 id=\"example-usage\">Example Usage</h2><p>terraform<br />resource \"aws_s3_bucket\" \"example_source\" {<br />  bucket = \"example-source\"<br />}</p><br /><p>data \"aws_iam_policy_document\" \"example_source\" {<br />  statement {<br />    sid    = \"AllowAppFlowSourceActions\"<br />    effect = \"Allow\"</p><br /><pre><code>principals {<br />  type        = \"Service\"<br />  identifiers = [\"appflow.amazonaws.com\"]<br />}<br /><br />actions = [<br />  \"s3:ListBucket\",<br />  \"s3:GetObject\",<br />]<br /><br />resources = [<br />  \"arn:aws:s3:::example_source\",<br />  \"arn:aws:s3:::example_source/*\",<br />]<br /></code></pre><br /><p>}<br />}</p><br /><p>resource \"aws_s3_bucket_policy\" \"example_source\" {<br />  bucket = aws_s3_bucket.example_source.id<br />  policy = data.aws_iam_policy_document.example_source.json<br />}</p><br /><p>resource \"aws_s3_object\" \"example\" {<br />  bucket = aws_s3_bucket.example_source.id<br />  key    = \"example_source.csv\"<br />  source = \"example_source.csv\"<br />}</p><br /><p>resource \"aws_s3_bucket\" \"example_destination\" {<br />  bucket = \"example-destination\"<br />}</p><br /><p>data \"aws_iam_policy_document\" \"example_destination\" {<br />  statement {<br />    sid    = \"AllowAppFlowDestinationActions\"<br />    effect = \"Allow\"</p><br /><pre><code>principals {<br />  type        = \"Service\"<br />  identifiers = [\"appflow.amazonaws.com\"]<br />}<br /><br />actions = [<br />  \"s3:PutObject\",<br />  \"s3:AbortMultipartUpload\",<br />  \"s3:ListMultipartUploadParts\",<br />  \"s3:ListBucketMultipartUploads\",<br />  \"s3:GetBucketAcl\",<br />  \"s3:PutObjectAcl\",<br />]<br /><br />resources = [<br />  \"arn:aws:s3:::example_destination\",<br />  \"arn:aws:s3:::example_destination/*\",<br />]<br /></code></pre><br /><p>}<br />}</p><br /><p>resource \"aws_s3_bucket_policy\" \"example_destination\" {<br />  bucket = aws_s3_bucket.example_destination.id<br />  policy = data.aws_iam_policy_document.example_destination.json<br />}</p><br /><p>resource \"aws_appflow_flow\" \"example\" {<br />  name = \"example\"</p><br /><p>source_flow_config {<br />    connector_type = \"S3\"<br />    source_connector_properties {<br />      s3 {<br />        bucket_name   = aws_s3_bucket_policy.example_source.bucket<br />        bucket_prefix = \"example\"<br />      }<br />    }<br />  }</p><br /><p>destination_flow_config {<br />    connector_type = \"S3\"<br />    destination_connector_properties {<br />      s3 {<br />        bucket_name = aws_s3_bucket_policy.example_destination.bucket</p><br /><pre><code>    s3_output_format_config {<br />      prefix_config {<br />        prefix_type = \"PATH\"<br />      }<br />    }<br />  }<br />}<br /></code></pre><br /><p>}</p><br /><p>task {<br />    source_fields     = [\"exampleField\"]<br />    destination_field = \"exampleField\"<br />    task_type         = \"Map\"</p><br /><pre><code>connector_operator {<br />  s3 = \"NO_OP\"<br />}<br /></code></pre><br /><p>}</p><br /><p>trigger_config {<br />    trigger_type = \"OnDemand\"<br />  }<br />}</p><br />", "argument-reference": "<h2 id=\"argument-reference\">Argument Reference</h2><p>This resource supports the following arguments:</p><ul><li><code>name</code> - (Required) Name of the flow.</li><li><code>destination_flow_config</code> - (Required) A <a href=\"#destination-flow-config\">Destination Flow Config</a> that controls how Amazon AppFlow places data in the destination connector.</li><li><code>source_flow_config</code> - (Required) The <a href=\"#source-flow-config\">Source Flow Config</a> that controls how Amazon AppFlow retrieves data from the source connector.</li><li><code>task</code> - (Required) A <a href=\"#task\">Task</a> that Amazon AppFlow performs while transferring the data in the flow run.</li><li><code>trigger_config</code> - (Required) A <a href=\"#trigger-config\">Trigger</a> that determine how and when the flow runs.</li><li><code>description</code> - (Optional) Description of the flow you want to create.</li><li><code>kms_arn</code> - (Optional) ARN (Amazon Resource Name) of the Key Management Service (KMS) key you provide for encryption. This is required if you do not want to use the Amazon AppFlow-managed KMS key. If you don't provide anything here, Amazon AppFlow uses the Amazon AppFlow-managed KMS key.</li><li><code>tags</code> - (Optional) Key-value mapping of resource tags. If configured with a provider <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs#default_tags-configuration-block\"><code>default_tags</code> configuration block</a> present, tags with matching keys will overwrite those defined at the provider-level.</li><li><code>tags_all</code> - Map of tags assigned to the resource, including those inherited from the provider <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs#default_tags-configuration-block\"><code>default_tags</code> configuration block</a>.</li></ul><h3 id=\"destination-flow-config\">Destination Flow Config</h3><ul><li><code>connector_type</code> - (Required) Type of connector, such as Salesforce, Amplitude, and so on. Valid values are <code>Salesforce</code>, <code>Singular</code>, <code>Slack</code>, <code>Redshift</code>, <code>S3</code>, <code>Marketo</code>, <code>Googleanalytics</code>, <code>Zendesk</code>, <code>Servicenow</code>, <code>Datadog</code>, <code>Trendmicro</code>, <code>Snowflake</code>, <code>Dynatrace</code>, <code>Infornexus</code>, <code>Amplitude</code>, <code>Veeva</code>, <code>EventBridge</code>, <code>LookoutMetrics</code>, <code>Upsolver</code>, <code>Honeycode</code>, <code>CustomerProfiles</code>, <code>SAPOData</code>, and <code>CustomConnector</code>.</li><li><code>destination_connector_properties</code> - (Required) This stores the information that is required to query a particular connector. See <a href=\"#destination-connector-properties\">Destination Connector Properties</a> for more information.</li><li><code>api_version</code> - (Optional) API version that the destination connector uses.</li><li><code>connector_profile_name</code> - (Optional) Name of the connector profile. This name must be unique for each connector profile in the AWS account.</li></ul><h4 id=\"destination-connector-properties\">Destination Connector Properties</h4><ul><li><code>custom_connector</code> - (Optional) Properties that are required to query the custom Connector. See <a href=\"#custom-connector-destination-properties\">Custom Connector Destination Properties</a> for more details.</li><li><code>customer_profiles</code> - (Optional) Properties that are required to query Amazon Connect Customer Profiles. See <a href=\"#customer-profiles-destination-properties\">Customer Profiles Destination Properties</a> for more details.</li><li><code>event_bridge</code> - (Optional) Properties that are required to query Amazon EventBridge. See <a href=\"#generic-destination-properties\">Generic Destination Properties</a> for more details.</li><li><code>honeycode</code> - (Optional) Properties that are required to query Amazon Honeycode. See <a href=\"#generic-destination-properties\">Generic Destination Properties</a> for more details.</li><li><code>marketo</code> - (Optional) Properties that are required to query Marketo. See <a href=\"#generic-destination-properties\">Generic Destination Properties</a> for more details.</li><li><code>redshift</code> - (Optional) Properties that are required to query Amazon Redshift. See <a href=\"#redshift-destination-properties\">Redshift Destination Properties</a> for more details.</li><li><code>s3</code> - (Optional) Properties that are required to query Amazon S3. See <a href=\"#s3-destination-properties\">S3 Destination Properties</a> for more details.</li><li><code>salesforce</code> - (Optional) Properties that are required to query Salesforce. See <a href=\"#salesforce-destination-properties\">Salesforce Destination Properties</a> for more details.</li><li><code>sapo_data</code> - (Optional) Properties that are required to query SAPOData. See <a href=\"#sapodata-destination-properties\">SAPOData Destination Properties</a> for more details.</li><li><code>snowflake</code> - (Optional) Properties that are required to query Snowflake. See <a href=\"#snowflake-destination-properties\">Snowflake Destination Properties</a> for more details.</li><li><code>upsolver</code> - (Optional) Properties that are required to query Upsolver. See <a href=\"#upsolver-destination-properties\">Upsolver Destination Properties</a> for more details.</li><li><code>zendesk</code> - (Optional) Properties that are required to query Zendesk. See <a href=\"#zendesk-destination-properties\">Zendesk Destination Properties</a> for more details.</li></ul><h5 id=\"generic-destination-properties\">Generic Destination Properties</h5><p>EventBridge, Honeycode, and Marketo destination properties all support the following attributes:</p><ul><li><code>object</code> - (Required) Object specified in the flow destination.</li><li><code>error_handling_config</code> - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See <a href=\"#error-handling-config\">Error Handling Config</a> for more details.</li></ul><h5 id=\"custom-connector-destination-properties\">Custom Connector Destination Properties</h5><ul><li><code>entity_name</code> - (Required) Entity specified in the custom connector as a destination in the flow.</li><li><code>custom_properties</code> - (Optional) Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.</li><li><code>error_handling_config</code> - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the custom connector as destination. See <a href=\"#error-handling-config\">Error Handling Config</a> for more details.</li><li><code>id_field_names</code> - (Optional) Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.</li><li><code>write_operation_type</code> - (Optional) Type of write operation to be performed in the custom connector when it's used as destination. Valid values are <code>INSERT</code>, <code>UPSERT</code>, <code>UPDATE</code>, and <code>DELETE</code>.</li></ul><h5 id=\"customer-profiles-destination-properties\">Customer Profiles Destination Properties</h5><ul><li><code>domain_name</code> - (Required) Unique name of the Amazon Connect Customer Profiles domain.</li><li><code>object_type_name</code> - (Optional) Object specified in the Amazon Connect Customer Profiles flow destination.</li></ul><h5 id=\"redshift-destination-properties\">Redshift Destination Properties</h5><ul><li><code>intermediate_bucket_name</code> - (Required) Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.</li><li><code>object</code> - (Required) Object specified in the Amazon Redshift flow destination.</li><li><code>bucket_prefix</code> - (Optional) Object key for the bucket in which Amazon AppFlow places the destination files.</li><li><code>error_handling_config</code> - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See <a href=\"#error-handling-config\">Error Handling Config</a> for more details.</li></ul><h5 id=\"s3-destination-properties\">S3 Destination Properties</h5><ul><li><code>bucket_name</code> - (Required) Amazon S3 bucket name in which Amazon AppFlow places the transferred data.</li><li><code>bucket_prefix</code> - (Optional) Object key for the bucket in which Amazon AppFlow places the destination files.</li><li><code>s3_output_format_config</code> - (Optional) Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See <a href=\"#s3-output-format-config\">S3 Output Format Config</a> for more details.</li></ul><h6 id=\"s3-output-format-config\">S3 Output Format Config</h6><ul><li><code>aggregation_config</code> - (Optional) Aggregation settings that you can use to customize the output format of your flow data. See <a href=\"#aggregation-config\">Aggregation Config</a> for more details.</li><li><code>file_type</code> - (Optional) File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are <code>CSV</code>, <code>JSON</code>, and <code>PARQUET</code>.</li><li><code>prefix_config</code> - (Optional) Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See <a href=\"#prefix-config\">Prefix Config</a> for more details.</li><li><code>preserve_source_data_typing</code> - (Optional, Boolean) Whether the data types from the source system need to be preserved (Only valid for <code>Parquet</code> file type)</li></ul><h5 id=\"salesforce-destination-properties\">Salesforce Destination Properties</h5><ul><li><code>object</code> - (Required) Object specified in the flow destination.</li><li><code>error_handling_config</code> - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See <a href=\"#error-handling-config\">Error Handling Config</a> for more details.</li><li><code>id_field_names</code> - (Optional) Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.</li><li><code>write_operation_type</code> - (Optional) This specifies the type of write operation to be performed in Salesforce. When the value is <code>UPSERT</code>, then <code>id_field_names</code> is required. Valid values are <code>INSERT</code>, <code>UPSERT</code>, <code>UPDATE</code>, and <code>DELETE</code>.</li></ul><h5 id=\"sapodata-destination-properties\">SAPOData Destination Properties</h5><ul><li><code>object_path</code> - (Required) Object path specified in the SAPOData flow destination.</li><li><code>error_handling_config</code> - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See <a href=\"#error-handling-config\">Error Handling Config</a> for more details.</li><li><code>id_field_names</code> - (Optional) Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.</li><li><code>success_response_handling_config</code> - (Optional) Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See <a href=\"#success-response-handling-config\">Success Response Handling Config</a> for more details.</li><li><code>write_operation</code> - (Optional) Possible write operations in the destination connector. When this value is not provided, this defaults to the <code>INSERT</code> operation. Valid values are <code>INSERT</code>, <code>UPSERT</code>, <code>UPDATE</code>, and <code>DELETE</code>.</li></ul><h6 id=\"success-response-handling-config\">Success Response Handling Config</h6><ul><li><code>bucket_name</code> - (Optional) Name of the Amazon S3 bucket.</li><li><code>bucket_prefix</code> - (Optional) Amazon S3 bucket prefix.</li></ul><h5 id=\"snowflake-destination-properties\">Snowflake Destination Properties</h5><ul><li><code>intermediate_bucket_name</code> - (Required) Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Snowflake.</li><li><code>object</code> - (Required) Object specified in the Amazon Snowflake flow destination.</li><li><code>bucket_prefix</code> - (Optional) Object key for the bucket in which Amazon AppFlow places the destination files.</li><li><code>error_handling_config</code> - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See <a href=\"#error-handling-config\">Error Handling Config</a> for more details.</li></ul><h5 id=\"upsolver-destination-properties\">Upsolver Destination Properties</h5><ul><li><code>bucket_name</code> - (Required) Upsolver Amazon S3 bucket name in which Amazon AppFlow places the transferred data. This must begin with <code>upsolver-appflow</code>.</li><li><code>bucket_prefix</code> - (Optional) Object key for the Upsolver Amazon S3 Bucket in which Amazon AppFlow places the destination files.</li><li><code>s3_output_format_config</code> - (Optional) Configuration that determines how Amazon AppFlow should format the flow output data when Upsolver is used as the destination. See <a href=\"#upsolver-s3-output-format-config\">Upsolver S3 Output Format Config</a> for more details.</li></ul><h6 id=\"upsolver-s3-output-format-config\">Upsolver S3 Output Format Config</h6><ul><li><code>aggregation_config</code> - (Optional) Aggregation settings that you can use to customize the output format of your flow data. See <a href=\"#aggregation-config\">Aggregation Config</a> for more details.</li><li><code>file_type</code> - (Optional) File type that Amazon AppFlow places in the Upsolver Amazon S3 bucket. Valid values are <code>CSV</code>, <code>JSON</code>, and <code>PARQUET</code>.</li><li><code>prefix_config</code> - (Optional) Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See <a href=\"#prefix-config\">Prefix Config</a> for more details.</li></ul><h6 id=\"aggregation-config\">Aggregation Config</h6><ul><li><code>aggregation_type</code> - (Optional) Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are <code>None</code> and <code>SingleFile</code>.</li></ul><h6 id=\"prefix-config\">Prefix Config</h6><ul><li><code>prefix_format</code> - (Optional) Determines the level of granularity that's included in the prefix. Valid values are <code>YEAR</code>, <code>MONTH</code>, <code>DAY</code>, <code>HOUR</code>, and <code>MINUTE</code>.</li><li><code>prefix_type</code> - (Optional) Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are <code>FILENAME</code>, <code>PATH</code>, and <code>PATH_AND_FILENAME</code>.</li></ul><h5 id=\"zendesk-destination-properties\">Zendesk Destination Properties</h5><ul><li><code>object</code> - (Required) Object specified in the flow destination.</li><li><code>error_handling_config</code> - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See <a href=\"#error-handling-config\">Error Handling Config</a> for more details.</li><li><code>id_field_names</code> - (Optional) Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.</li><li><code>write_operation_type</code> - (Optional) This specifies the type of write operation to be performed in Zendesk. When the value is <code>UPSERT</code>, then <code>id_field_names</code> is required. Valid values are <code>INSERT</code>, <code>UPSERT</code>, <code>UPDATE</code>, and <code>DELETE</code>.</li></ul><h6 id=\"error-handling-config\">Error Handling Config</h6><ul><li><code>bucket_name</code> - (Optional) Name of the Amazon S3 bucket.</li><li><code>bucket_prefix</code> - (Optional) Amazon S3 bucket prefix.</li><li><code>fail_on_first_destination_error</code> - (Optional, boolean) If the flow should fail after the first instance of a failure when attempting to place data in the destination.</li></ul><h3 id=\"source-flow-config\">Source Flow Config</h3><ul><li><code>connector_type</code> - (Required) Type of connector, such as Salesforce, Amplitude, and so on. Valid values are <code>Salesforce</code>, <code>Singular</code>, <code>Slack</code>, <code>Redshift</code>, <code>S3</code>, <code>Marketo</code>, <code>Googleanalytics</code>, <code>Zendesk</code>, <code>Servicenow</code>, <code>Datadog</code>, <code>Trendmicro</code>, <code>Snowflake</code>, <code>Dynatrace</code>, <code>Infornexus</code>, <code>Amplitude</code>, <code>Veeva</code>, <code>EventBridge</code>, <code>LookoutMetrics</code>, <code>Upsolver</code>, <code>Honeycode</code>, <code>CustomerProfiles</code>, <code>SAPOData</code>, and <code>CustomConnector</code>.</li><li><code>source_connector_properties</code> - (Required) Information that is required to query a particular source connector. See <a href=\"#source-connector-properties\">Source Connector Properties</a> for details.</li><li><code>api_version</code> - (Optional) API version that the destination connector uses.</li><li><code>connector_profile_name</code> - (Optional) Name of the connector profile. This name must be unique for each connector profile in the AWS account.</li><li><code>incremental_pull_config</code> - (Optional) Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See <a href=\"#incremental-pull-config\">Incremental Pull Config</a> for more details.</li></ul><h4 id=\"source-connector-properties\">Source Connector Properties</h4><ul><li><code>amplitude</code> - (Optional) Information that is required for querying Amplitude. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>custom_connector</code> - (Optional) Properties that are applied when the custom connector is being used as a source. See <a href=\"#custom-connector-source-properties\">Custom Connector Source Properties</a>.</li><li><code>datadog</code> - (Optional) Information that is required for querying Datadog. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>dynratrace</code> - (Optional) Information that is required for querying Dynatrace. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>infor_nexus</code> - (Optional) Information that is required for querying Infor Nexus. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>marketo</code> - (Optional) Information that is required for querying Marketo. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>s3</code> - (Optional) Information that is required for querying Amazon S3. See <a href=\"#s3-source-properties\">S3 Source Properties</a> for more details.</li><li><code>salesforce</code> - (Optional) Information that is required for querying Salesforce. See <a href=\"#s3-source-properties\">Salesforce Source Properties</a> for more details.</li><li><code>sapo_data</code> - (Optional) Information that is required for querying SAPOData as a flow source. See <a href=\"#sapodata-source-properties\">SAPO Source Properties</a> for more details.</li><li><code>service_now</code> - (Optional) Information that is required for querying ServiceNow. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>singular</code> - (Optional) Information that is required for querying Singular. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>slack</code> - (Optional) Information that is required for querying Slack. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>trend_micro</code> - (Optional) Information that is required for querying Trend Micro. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li><li><code>veeva</code> - (Optional) Information that is required for querying Veeva. See <a href=\"#veeva-source-properties\">Veeva Source Properties</a> for more details.</li><li><code>zendesk</code> - (Optional) Information that is required for querying Zendesk. See <a href=\"#generic-source-properties\">Generic Source Properties</a> for more details.</li></ul><h5 id=\"generic-source-properties\">Generic Source Properties</h5><p>Amplitude, Datadog, Dynatrace, Google Analytics, Infor Nexus, Marketo, ServiceNow, Singular, Slack, Trend Micro, and Zendesk source properties all support the following attributes:</p><ul><li><code>object</code> - (Required) Object specified in the flow source.</li></ul><h5 id=\"custom-connector-source-properties\">Custom Connector Source Properties</h5><ul><li><code>entity_name</code> - (Required) Entity specified in the custom connector as a source in the flow.</li><li><code>custom_properties</code> - (Optional) Custom properties that are specific to the connector when it's used as a source in the flow. Maximum of 50 items.</li></ul><h5 id=\"s3-source-properties\">S3 Source Properties</h5><ul><li><code>bucket_name</code> - (Required) Amazon S3 bucket name where the source files are stored.</li><li><code>bucket_prefix</code> - (Optional) Object key for the Amazon S3 bucket in which the source files are stored.</li><li><code>s3_input_format_config</code> - (Optional) When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See <a href=\"#s3-input-format-config\">S3 Input Format Config</a> for details.</li></ul><h6 id=\"s3-input-format-config\">S3 Input Format Config</h6><ul><li><code>s3_input_file_type</code> - (Optional) File type that Amazon AppFlow gets from your Amazon S3 bucket. Valid values are <code>CSV</code> and <code>JSON</code>.</li></ul><h5 id=\"salesforce-source-properties\">Salesforce Source Properties</h5><ul><li><code>object</code> - (Required) Object specified in the Salesforce flow source.</li><li><code>enable_dynamic_field_update</code> - (Optional, boolean) Flag that enables dynamic fetching of new (recently added) fields in the Salesforce objects while running a flow.</li><li><code>include_deleted_records</code> - (Optional, boolean) Whether Amazon AppFlow includes deleted files in the flow run.</li></ul><h5 id=\"sapodata-source-properties\">SAPOData Source Properties</h5><ul><li><code>object_path</code> - (Required) Object path specified in the SAPOData flow source.</li></ul><h5 id=\"veeva-source-properties\">Veeva Source Properties</h5><ul><li><code>object</code> - (Required) Object specified in the Veeva flow source.</li><li><code>document_type</code> - (Optional) Document type specified in the Veeva document extract flow.</li><li><code>include_all_versions</code> - (Optional, boolean) Boolean value to include All Versions of files in Veeva document extract flow.</li><li><code>include_renditions</code> - (Optional, boolean) Boolean value to include file renditions in Veeva document extract flow.</li><li><code>include_source_files</code> - (Optional, boolean) Boolean value to include source files in Veeva document extract flow.</li></ul><h4 id=\"incremental-pull-config\">Incremental Pull Config</h4><ul><li><code>datetime_type_field_name</code> - (Optional) Field that specifies the date time or timestamp field as the criteria to use when importing incremental records from the source.</li></ul><h3 id=\"task\">Task</h3><ul><li><code>source_fields</code> - (Required) Source fields to which a particular task is applied.</li><li><code>task_type</code> - (Required) Particular task implementation that Amazon AppFlow performs. Valid values are <code>Arithmetic</code>, <code>Filter</code>, <code>Map</code>, <code>Map_all</code>, <code>Mask</code>, <code>Merge</code>, <code>Passthrough</code>, <code>Truncate</code>, and <code>Validate</code>.</li><li><code>connector_operator</code> - (Optional) Operation to be performed on the provided source fields. See <a href=\"#connector-operator\">Connector Operator</a> for details.</li><li><code>destination_field</code> - (Optional) Field in a destination connector, or a field value against which Amazon AppFlow validates a source field.</li><li><code>task_properties</code> - (Optional) Map used to store task-related information. The execution service looks for particular information based on the <code>TaskType</code>. Valid keys are <code>VALUE</code>, <code>VALUES</code>, <code>DATA_TYPE</code>, <code>UPPER_BOUND</code>, <code>LOWER_BOUND</code>, <code>SOURCE_DATA_TYPE</code>, <code>DESTINATION_DATA_TYPE</code>, <code>VALIDATION_ACTION</code>, <code>MASK_VALUE</code>, <code>MASK_LENGTH</code>, <code>TRUNCATE_LENGTH</code>, <code>MATH_OPERATION_FIELDS_ORDER</code>, <code>CONCAT_FORMAT</code>, <code>SUBFIELD_CATEGORY_MAP</code>, and <code>EXCLUDE_SOURCE_FIELDS_LIST</code>.</li></ul><h4 id=\"connector-operator\">Connector Operator</h4><ul><li><code>amplitude</code> - (Optional) Operation to be performed on the provided Amplitude source fields. The only valid value is <code>BETWEEN</code>.</li><li><code>custom_connector</code> - (Optional) Operators supported by the custom connector. Valid values are <code>PROJECTION</code>, <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>CONTAINS</code>, <code>BETWEEN</code>, <code>LESS_THAN_OR_EQUAL_TO</code>, <code>GREATER_THAN_OR_EQUAL_TO</code>, <code>EQUAL_TO</code>, <code>NOT_EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>datadog</code> - (Optional) Operation to be performed on the provided Datadog source fields. Valid values are <code>PROJECTION</code>, <code>BETWEEN</code>, <code>EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>dynatrace</code> - (Optional) Operation to be performed on the provided Dynatrace source fields. Valid values are <code>PROJECTION</code>, <code>BETWEEN</code>, <code>EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>google_analytics</code> - (Optional) Operation to be performed on the provided Google Analytics source fields. Valid values are <code>PROJECTION</code> and <code>BETWEEN</code>.</li><li><code>infor_nexus</code> - (Optional) Operation to be performed on the provided Infor Nexus source fields. Valid values are <code>PROJECTION</code>, <code>BETWEEN</code>, <code>EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>marketo</code> - (Optional) Operation to be performed on the provided Marketo source fields. Valid values are <code>PROJECTION</code>, <code>BETWEEN</code>, <code>EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>s3</code> - (Optional) Operation to be performed on the provided Amazon S3 source fields. Valid values are <code>PROJECTION</code>, <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>BETWEEN</code>, <code>LESS_THAN_OR_EQUAL_TO</code>, <code>GREATER_THAN_OR_EQUAL_TO</code>, <code>EQUAL_TO</code>, <code>NOT_EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>salesforce</code> - (Optional) Operation to be performed on the provided Salesforce source fields. Valid values are <code>PROJECTION</code>, <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>CONTAINS</code>, <code>BETWEEN</code>, <code>LESS_THAN_OR_EQUAL_TO</code>, <code>GREATER_THAN_OR_EQUAL_TO</code>, <code>EQUAL_TO</code>, <code>NOT_EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>sapo_data</code> - (Optional) Operation to be performed on the provided SAPOData source fields. Valid values are <code>PROJECTION</code>, <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>CONTAINS</code>, <code>BETWEEN</code>, <code>LESS_THAN_OR_EQUAL_TO</code>, <code>GREATER_THAN_OR_EQUAL_TO</code>, <code>EQUAL_TO</code>, <code>NOT_EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>service_now</code> - (Optional) Operation to be performed on the provided ServiceNow source fields. Valid values are <code>PROJECTION</code>, <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>CONTAINS</code>, <code>BETWEEN</code>, <code>LESS_THAN_OR_EQUAL_TO</code>, <code>GREATER_THAN_OR_EQUAL_TO</code>, <code>EQUAL_TO</code>, <code>NOT_EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>singular</code> - (Optional) Operation to be performed on the provided Singular source fields. Valid values are <code>PROJECTION</code>, <code>EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>slack</code> - (Optional) Operation to be performed on the provided Slack source fields. Valid values are <code>PROJECTION</code>, <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>BETWEEN</code>, <code>LESS_THAN_OR_EQUAL_TO</code>, <code>GREATER_THAN_OR_EQUAL_TO</code>, <code>EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>trendmicro</code> - (Optional) Operation to be performed on the provided Trend Micro source fields. Valid values are <code>PROJECTION</code>, <code>EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>veeva</code> - (Optional) Operation to be performed on the provided Veeva source fields. Valid values are <code>PROJECTION</code>, <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>CONTAINS</code>, <code>BETWEEN</code>, <code>LESS_THAN_OR_EQUAL_TO</code>, <code>GREATER_THAN_OR_EQUAL_TO</code>, <code>EQUAL_TO</code>, <code>NOT_EQUAL_TO</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li><li><code>zendesk</code> - (Optional) Operation to be performed on the provided Zendesk source fields. Valid values are <code>PROJECTION</code>, <code>GREATER_THAN</code>, <code>ADDITION</code>, <code>MULTIPLICATION</code>, <code>DIVISION</code>, <code>SUBTRACTION</code>, <code>MASK_ALL</code>, <code>MASK_FIRST_N</code>, <code>MASK_LAST_N</code>, <code>VALIDATE_NON_NULL</code>, <code>VALIDATE_NON_ZERO</code>, <code>VALIDATE_NON_NEGATIVE</code>, <code>VALIDATE_NUMERIC</code>, and <code>NO_OP</code>.</li></ul><h3 id=\"trigger-config\">Trigger Config</h3><ul><li><code>trigger_type</code> - (Required) Type of flow trigger. Valid values are <code>Scheduled</code>, <code>Event</code>, and <code>OnDemand</code>.</li><li><code>trigger_properties</code> - (Optional) Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the <code>Scheduled</code> trigger type. See <a href=\"#scheduled-trigger-properties\">Scheduled Trigger Properties</a> for details.</li></ul><h4 id=\"scheduled-trigger-properties\">Scheduled Trigger Properties</h4><p>The <code>trigger_properties</code> block only supports one attribute: <code>scheduled</code>, a block which in turn supports the following:</p><ul><li><code>schedule_expression</code> - (Required) Scheduling expression that determines the rate at which the schedule will run, for example <code>rate(5minutes)</code>.</li><li><code>data_pull_mode</code> - (Optional) Whether a scheduled flow has an incremental data transfer or a complete data transfer for each flow run. Valid values are <code>Incremental</code> and <code>Complete</code>.</li><li><code>first_execution_from</code> - (Optional) Date range for the records to import from the connector in the first flow run. Must be a valid RFC3339 timestamp.</li><li><code>schedule_end_time</code> - (Optional) Scheduled end time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.</li><li><code>schedule_offset</code> - (Optional) Optional offset that is added to the time interval for a schedule-triggered flow. Maximum value of 36000.</li><li><code>schedule_start_time</code> - (Optional) Scheduled start time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.</li><li><code>timezone</code> - (Optional) Time zone used when referring to the date and time of a scheduled-triggered flow, such as <code>America/New_York</code>.</li></ul><p>terraform<br />resource \"aws_appflow_flow\" \"example\" {<br />  # ... other configuration ...</p><p>trigger_config {<br />    scheduled {<br />      schedule_expression = \"rate(1minutes)\"<br />    }<br />  }<br />}</p>", "attribute-reference": "<h2 id=\"attribute-reference\">Attribute Reference</h2><p>This resource exports the following attributes in addition to the arguments above:</p><ul><li><code>arn</code> - Flow's ARN.</li></ul>", "import": "<h2 id=\"import\">Import</h2><p>In Terraform v1.5.0 and later, use an <a href=\"https://developer.hashicorp.com/terraform/language/import\"><code>import</code> block</a> to import AppFlow flows using the <code>arn</code>. For example:</p><p>terraform<br />import {<br />  to = aws_appflow_flow.example<br />  id = \"arn:aws:appflow:us-west-2:123456789012:flow/example-flow\"<br />}</p><p>Using <code>terraform import</code>, import AppFlow flows using the <code>arn</code>. For example:</p><p>console<br />% terraform import aws_appflow_flow.example arn:aws:appflow:us-west-2:123456789012:flow/example-flow</p>", "description": "<h1 id=\"resource-aws_appflow_flow\">Resource: aws_appflow_flow</h1><p>Provides an AppFlow flow resource.</p>"}