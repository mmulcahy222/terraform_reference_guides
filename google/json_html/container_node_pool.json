{"google-container-node-pool": "<h1 id=\"google_container_node_pool\">google_container_node_pool</h1><p>-&gt; See the <a href=\"/docs/providers/google/guides/using_gke_with_terraform.html\">Using GKE with Terraform</a><br />guide for more information about using GKE with Terraform.</p><p>Manages a node pool in a Google Kubernetes Engine (GKE) cluster separately from<br />the cluster control plane. For more information see <a href=\"https://cloud.google.com/container-engine/docs/node-pools\">the official documentation</a><br />and <a href=\"https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters.nodePools\">the API reference</a>.</p><h3 id=\"example-usage-using-a-separately-managed-node-pool-recommended\">Example Usage - using a separately managed node pool (recommended)</h3><p><br />resource \"google_service_account\" \"default\" {<br />  account_id   = \"service-account-id\"<br />  display_name = \"Service Account\"<br />}</p><p>resource \"google_container_cluster\" \"primary\" {<br />  name     = \"my-gke-cluster\"<br />  location = \"us-central1\"</p><p># We can't create a cluster with no node pool defined, but we want to only use<br />  # separately managed node pools. So we create the smallest possible default<br />  # node pool and immediately delete it.<br />  remove_default_node_pool = true<br />  initial_node_count       = 1<br />}</p><p>resource \"google_container_node_pool\" \"primary_preemptible_nodes\" {<br />  name       = \"my-node-pool\"<br />  cluster    = google_container_cluster.primary.id<br />  node_count = 1</p><p>node_config {<br />    preemptible  = true<br />    machine_type = \"e2-medium\"</p><pre><code># Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.service_account = google_service_account.default.emailoauth_scopes = [  \"https://www.googleapis.com/auth/cloud-platform\"]</code></pre><p>}<br />}<br /></p><h3 id=\"example-usage-2-node-pools-1-separately-managed-the-default-node-pool\">Example Usage - 2 node pools, 1 separately managed + the default node pool</h3><p><br />resource \"google_service_account\" \"default\" {<br />  account_id   = \"service-account-id\"<br />  display_name = \"Service Account\"<br />}</p><p>resource \"google_container_node_pool\" \"np\" {<br />  name       = \"my-node-pool\"<br />  cluster    = google_container_cluster.primary.id<br />  node_config {<br />    machine_type = \"e2-medium\"<br />    # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.<br />    service_account = google_service_account.default.email<br />    oauth_scopes    = [<br />      \"https://www.googleapis.com/auth/cloud-platform\"<br />    ]<br />  }<br />  timeouts {<br />    create = \"30m\"<br />    update = \"20m\"<br />  }<br />}</p><p>resource \"google_container_cluster\" \"primary\" {<br />  name               = \"marcellus-wallace\"<br />  location           = \"us-central1-a\"<br />  initial_node_count = 3</p><p>node_locations = [<br />    \"us-central1-c\",<br />  ]</p><p>node_config {<br />    # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.<br />    service_account = google_service_account.default.email<br />    oauth_scopes = [<br />      \"https://www.googleapis.com/auth/cloud-platform\"<br />    ]<br />    guest_accelerator {<br />      type  = \"nvidia-tesla-k80\"<br />      count = 1<br />    }<br />  }<br />}<br /></p>", "argument-reference": "<h2 id=\"argument-reference\">Argument Reference</h2><ul><li><code>cluster</code> - (Required) The cluster to create the node pool for. Cluster must be present in <code>location</code> provided for clusters. May be specified in the format <code>projects/{{project}}/locations/{{location}}/clusters/{{cluster}}</code> or as just the name of the cluster.</li></ul><hr /><ul><li><code>location</code> - (Optional) The location (region or zone) of the cluster.</li></ul><hr /><ul><li><p><code>autoscaling</code> - (Optional) Configuration required by cluster autoscaler to adjust<br />    the size of the node pool to the current cluster usage. Structure is <a href=\"#nested_autoscaling\">documented below</a>.</p></li><li><p><code>confidential_nodes</code> - (Optional) Configuration for Confidential Nodes feature. Structure is <a href=\"#nested_confidential_nodes\">documented below</a>.</p></li><li><p><code>initial_node_count</code> - (Optional) The initial number of nodes for the pool. In<br />    regional or multi-zonal clusters, this is the number of nodes per zone. Changing<br />    this will force recreation of the resource. WARNING: Resizing your node pool manually<br />    may change this value in your existing cluster, which will trigger destruction<br />    and recreation on the next Terraform run (to rectify the discrepancy).  If you don't<br />    need this value, don't set it.  If you do need it, you can <a href=\"https://github.com/hashicorp/terraform-provider-google/issues/6901#issuecomment-667369691\">use a lifecycle block to<br />    ignore subsequent changes to this field</a>.</p></li><li><p><code>management</code> - (Optional) Node management configuration, wherein auto-repair and<br />    auto-upgrade is configured. Structure is <a href=\"#nested_management\">documented below</a>.</p></li><li><p><code>max_pods_per_node</code> - (Optional) The maximum number of pods per node in this node pool.<br />    Note that this does not work on node pools which are \"route-based\" - that is, node<br />    pools belonging to clusters that do not have IP Aliasing enabled.<br />    See the <a href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr\">official documentation</a><br />    for more information.</p></li><li><p><code>node_locations</code> - (Optional)<br />The list of zones in which the node pool's nodes should be located. Nodes must<br />be in the region of their regional cluster or in the same region as their<br />cluster's zone for zonal clusters. If unspecified, the cluster-level<br /><code>node_locations</code> will be used.</p></li></ul><p>-&gt; Note: <code>node_locations</code> will not revert to the cluster's default set of zones<br />upon being unset. You must manually reconcile the list of zones with your<br />cluster.</p><ul><li><p><code>name</code> - (Optional) The name of the node pool. If left blank, Terraform will<br />    auto-generate a unique name.</p></li><li><p><code>name_prefix</code> - (Optional) Creates a unique name for the node pool beginning<br />    with the specified prefix. Conflicts with <code>name</code>.</p></li><li><p><code>node_config</code> - (Optional) Parameters used in creating the node pool. See<br /><a href=\"container_cluster.html#nested_node_config\">google_container_cluster</a> for schema.</p></li><li><p><code>network_config</code> - (Optional) The network configuration of the pool. Such as<br />    configuration for <a href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr\">Adding Pod IP address ranges</a>) to the node pool. Or enabling private nodes. Structure is<br /><a href=\"#nested_network_config\">documented below</a></p></li><li><p><code>node_count</code> - (Optional) The number of nodes per instance group. This field can be used to<br />    update the number of nodes per instance group but should not be used alongside <code>autoscaling</code>.</p></li><li><p><code>project</code> - (Optional) The ID of the project in which to create the node pool. If blank,<br />    the provider-configured project will be used.</p></li><li><p><code>upgrade_settings</code> (Optional) Specify node upgrade settings to change how GKE upgrades nodes.<br />    The maximum number of nodes upgraded simultaneously is limited to 20. Structure is <a href=\"#nested_upgrade_settings\">documented below</a>.</p></li><li><p><code>version</code> - (Optional) The Kubernetes version for the nodes in this pool. Note that if this field<br />    and <code>auto_upgrade</code> are both specified, they will fight each other for what the node version should<br />    be, so setting both is highly discouraged. While a fuzzy version can be specified, it's<br />    recommended that you specify explicit versions as Terraform will see spurious diffs<br />    when fuzzy versions are used. See the <code>google_container_engine_versions</code> data source's<br /><code>version_prefix</code> field to approximate fuzzy versions in a Terraform-compatible way.</p></li><li><p><code>placement_policy</code> - (Optional) Specifies a custom placement policy for the<br />  nodes.</p></li></ul><p><a name=\"nested_autoscaling\"></a>The <code>autoscaling</code> block supports (either total or per zone limits are required):</p><ul><li><p><code>min_node_count</code> - (Optional) Minimum number of nodes per zone in the NodePool.<br />    Must be &gt;=0 and &lt;= <code>max_node_count</code>. Cannot be used with total limits.</p></li><li><p><code>max_node_count</code> - (Optional) Maximum number of nodes per zone in the NodePool.<br />    Must be &gt;= min_node_count. Cannot be used with total limits.</p></li><li><p><code>total_min_node_count</code> - (Optional) Total minimum number of nodes in the NodePool.<br />    Must be &gt;=0 and &lt;= <code>total_max_node_count</code>. Cannot be used with per zone limits.<br />    Total size limits are supported only in 1.24.1+ clusters.</p></li><li><p><code>total_max_node_count</code> - (Optional) Total maximum number of nodes in the NodePool.<br />    Must be &gt;= total_min_node_count. Cannot be used with per zone limits.<br />    Total size limits are supported only in 1.24.1+ clusters.</p></li><li><p><code>location_policy</code> - (Optional) Location policy specifies the algorithm used when<br />  scaling-up the node pool. Location policy is supported only in 1.24.1+ clusters.</p><ul><li>\"BALANCED\" - Is a best effort policy that aims to balance the sizes of available zones.</li><li>\"ANY\" - Instructs the cluster autoscaler to prioritize utilization of unused reservations,<br />  and reduce preemption risk for Spot VMs.</li></ul></li></ul><p><a name=\"nested_confidential_nodes\"></a> The <code>confidential_nodes</code> block supports:</p><ul><li><code>enabled</code> (Required) - Enable Confidential GKE Nodes for this cluster, to<br />    enforce encryption of data in-use.</li></ul><p><a name=\"nested_management\"></a>The <code>management</code> block supports:</p><ul><li><p><code>auto_repair</code> - (Optional) Whether the nodes will be automatically repaired.</p></li><li><p><code>auto_upgrade</code> - (Optional) Whether the nodes will be automatically upgraded.</p></li></ul><p><a name=\"nested_network_config\"></a>The <code>network_config</code> block supports:</p><ul><li><p><code>create_pod_range</code> - (Optional) Whether to create a new range for pod IPs in this node pool. Defaults are provided for <code>pod_range</code> and <code>pod_ipv4_cidr_block</code> if they are not specified.</p></li><li><p><code>enable_private_nodes</code> - (Optional) Whether nodes have internal IP addresses only.</p></li><li><p><code>pod_ipv4_cidr_block</code> - (Optional) The IP address range for pod IPs in this node pool. Only applicable if createPodRange is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) to pick a specific range to use.</p></li><li><p><code>pod_range</code> - (Optional) The ID of the secondary range for pod IPs. If <code>create_pod_range</code> is true, this ID is used for the new range. If <code>create_pod_range</code> is false, uses an existing secondary range with this ID.</p></li><li><p><code>additional_node_network_configs</code> - (Optional, Beta) We specify the additional node networks for this node pool using this list. Each node network corresponds to an additional interface.<br />    Structure is <a href=\"#nested_additional_node_network_configs\">documented below</a></p></li><li><p><code>additional_pod_network_configs</code> - (Optional, Beta) We specify the additional pod networks for this node pool using this list. Each pod network corresponds to an additional alias IP range for the node.<br />    Structure is <a href=\"#nested_additional_pod_network_configs\">documented below</a></p></li></ul><p><a name=\"nested_additional_node_network_configs\"></a>The <code>additional_node_network_configs</code> block supports:</p><ul><li><p><code>network</code> - Name of the VPC where the additional interface belongs.</p></li><li><p><code>subnetwork</code> - Name of the subnetwork where the additional interface belongs.</p></li></ul><p><a name=\"nested_additional_pod_network_configs\"></a>The <code>additional_pod_network_configs</code> block supports:</p><ul><li><p><code>subnetwork</code> - Name of the subnetwork where the additional pod network belongs.</p></li><li><p><code>secondary_pod_range</code> - The name of the secondary range on the subnet which provides IP address for this pod range.</p></li><li><p><code>max_pods_per_node</code> - The maximum number of pods per node which use this pod network.</p></li></ul><p><a name=\"nested_upgrade_settings\"></a>The <code>upgrade_settings</code> block supports:</p><ul><li><p><code>max_surge</code> - (Optional) The number of additional nodes that can be added to the node pool during<br />    an upgrade. Increasing <code>max_surge</code> raises the number of nodes that can be upgraded simultaneously.<br />    Can be set to 0 or greater.</p></li><li><p><code>max_unavailable</code> - (Optional) The number of nodes that can be simultaneously unavailable during<br />    an upgrade. Increasing <code>max_unavailable</code> raises the number of nodes that can be upgraded in<br />    parallel. Can be set to 0 or greater.</p></li></ul><p><code>max_surge</code> and <code>max_unavailable</code> must not be negative and at least one of them must be greater than zero.</p><ul><li><p><code>strategy</code> - (Default <code>SURGE</code>) The upgrade stragey to be used for upgrading the nodes.</p></li><li><p><code>blue_green_settings</code> - (Optional) The settings to adjust <a href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy\">blue green upgrades</a>.<br />    Structure is <a href=\"#nested_blue_green_settings\">documented below</a></p></li></ul><p><a name=\"nested_blue_green_settings\"></a>The <code>blue_green_settings</code> block supports:</p><ul><li><p><code>standard_rollout_policy</code> - (Required) Specifies the standard policy settings for blue-green upgrades.</p><ul><li><code>batch_percentage</code> - (Optional) Percentage of the blue pool nodes to drain in a batch.</li><li><code>batch_node_count</code> - (Optional) Number of blue nodes to drain in a batch.</li><li><code>batch_soak_duration</code> - (Optionial) Soak time after each batch gets drained.</li></ul></li><li><p><code>node_pool_soak_duration</code> - (Optional) Time needed after draining the entire blue pool.<br />    After this period, the blue pool will be cleaned up.</p></li></ul><p><a name=\"nested_placement_policy\"></a>The <code>placement_policy</code> block supports:</p><ul><li><p><code>type</code> - (Required) The type of the policy. Supports a single value: COMPACT.<br />  Specifying COMPACT placement policy type places node pool's nodes in a closer<br />  physical proximity in order to reduce network latency between nodes.</p></li><li><p><code>policy_name</code> - (Optional) If set, refers to the name of a custom resource policy supplied by the user.<br />  The resource policy must be in the same project and region as the node pool.<br />  If not found, InvalidArgument error is returned.</p></li><li><p><code>tpu_topology</code> - (Optional, Beta) The <a href=\"https://cloud.google.com/tpu/docs/types-topologies#tpu_topologies\">TPU placement topology</a> for pod slice node pool.</p></li></ul>", "attributes-reference": "<h2 id=\"attributes-reference\">Attributes Reference</h2><p>In addition to the arguments listed above, the following computed attributes are exported:</p><ul><li><p><code>id</code> - an identifier for the resource with format <code>{{project}}/{{location}}/{{cluster}}/{{name}}</code></p></li><li><p><code>instance_group_urls</code> - The resource URLs of the managed instance groups associated with this node pool.</p></li><li><p><code>managed_instance_group_urls</code> - List of instance group URLs which have been assigned to this node pool.</p></li></ul><p><a id=\"timeouts\"></a></p>", "timeouts": "<h2 id=\"timeouts\">Timeouts</h2><p><code>google_container_node_pool</code> provides the following<br /><a href=\"https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts\">Timeouts</a> configuration options: configuration options:</p><ul><li><code>create</code> - (Default <code>30 minutes</code>) Used for adding node pools</li><li><code>update</code> - (Default <code>30 minutes</code>) Used for updates to node pools</li><li><code>delete</code> - (Default <code>30 minutes</code>) Used for removing node pools.</li></ul>", "import": "<h2 id=\"import\">Import</h2><p>Node pools can be imported using the <code>project</code>, <code>location</code>, <code>cluster</code> and <code>name</code>. If<br />the project is omitted, the project value in the provider configuration will be used. Examples:</p><p><br />$ terraform import google_container_node_pool.mainpool my-gcp-project/us-east1-a/my-cluster/main-pool</p><p>$ terraform import google_container_node_pool.mainpool us-east1/my-cluster/main-pool<br /></p>", "description": "<h1 id=\"google_container_node_pool\">google_container_node_pool</h1><p>-&gt; See the <a href=\"/docs/providers/google/guides/using_gke_with_terraform.html\">Using GKE with Terraform</a><br />guide for more information about using GKE with Terraform.</p><p>Manages a node pool in a Google Kubernetes Engine (GKE) cluster separately from<br />the cluster control plane. For more information see <a href=\"https://cloud.google.com/container-engine/docs/node-pools\">the official documentation</a><br />and <a href=\"https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters.nodePools\">the API reference</a>.</p><h3 id=\"example-usage-using-a-separately-managed-node-pool-recommended\">Example Usage - using a separately managed node pool (recommended)</h3><p><br />resource \"google_service_account\" \"default\" {<br />  account_id   = \"service-account-id\"<br />  display_name = \"Service Account\"<br />}</p><p>resource \"google_container_cluster\" \"primary\" {<br />  name     = \"my-gke-cluster\"<br />  location = \"us-central1\"</p><p># We can't create a cluster with no node pool defined, but we want to only use<br />  # separately managed node pools. So we create the smallest possible default<br />  # node pool and immediately delete it.<br />  remove_default_node_pool = true<br />  initial_node_count       = 1<br />}</p><p>resource \"google_container_node_pool\" \"primary_preemptible_nodes\" {<br />  name       = \"my-node-pool\"<br />  cluster    = google_container_cluster.primary.id<br />  node_count = 1</p><p>node_config {<br />    preemptible  = true<br />    machine_type = \"e2-medium\"</p><pre><code># Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.service_account = google_service_account.default.emailoauth_scopes = [  \"https://www.googleapis.com/auth/cloud-platform\"]</code></pre><p>}<br />}<br /></p><h3 id=\"example-usage-2-node-pools-1-separately-managed-the-default-node-pool\">Example Usage - 2 node pools, 1 separately managed + the default node pool</h3><p><br />resource \"google_service_account\" \"default\" {<br />  account_id   = \"service-account-id\"<br />  display_name = \"Service Account\"<br />}</p><p>resource \"google_container_node_pool\" \"np\" {<br />  name       = \"my-node-pool\"<br />  cluster    = google_container_cluster.primary.id<br />  node_config {<br />    machine_type = \"e2-medium\"<br />    # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.<br />    service_account = google_service_account.default.email<br />    oauth_scopes    = [<br />      \"https://www.googleapis.com/auth/cloud-platform\"<br />    ]<br />  }<br />  timeouts {<br />    create = \"30m\"<br />    update = \"20m\"<br />  }<br />}</p><p>resource \"google_container_cluster\" \"primary\" {<br />  name               = \"marcellus-wallace\"<br />  location           = \"us-central1-a\"<br />  initial_node_count = 3</p><p>node_locations = [<br />    \"us-central1-c\",<br />  ]</p><p>node_config {<br />    # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.<br />    service_account = google_service_account.default.email<br />    oauth_scopes = [<br />      \"https://www.googleapis.com/auth/cloud-platform\"<br />    ]<br />    guest_accelerator {<br />      type  = \"nvidia-tesla-k80\"<br />      count = 1<br />    }<br />  }<br />}<br /></p>"}