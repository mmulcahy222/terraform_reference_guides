{"": "<h1 id=\"-_2\">----------------------------------------------------------------------------</h1><p>subcategory: \"Dataplex\"<br />description: |-<br />  A Dataplex task represents the work that you want Dataplex to do on a schedule.</p><hr />", "auto-generated-code-type-mmv1": "<h1 id=\"auto-generated-code-type-mmv1\"><strong><em>     AUTO GENERATED CODE    </em></strong>    Type: MMv1     ***</h1>", "this-file-is-automatically-generated-by-magic-modules-and-manual": "<h1 id=\"this-file-is-automatically-generated-by-magic-modules-and-manual\">This file is automatically generated by Magic Modules and manual</h1>", "changes-will-be-clobbered-when-the-file-is-regenerated": "<h1 id=\"changes-will-be-clobbered-when-the-file-is-regenerated\">changes will be clobbered when the file is regenerated.</h1>", "please-read-more-about-how-to-change-this-file-in": "<h1 id=\"please-read-more-about-how-to-change-this-file-in\">Please read more about how to change this file in</h1>", "github-contributing-md": "<h1 id=\"githubcontributingmd\">.github/CONTRIBUTING.md.</h1>", "google-dataplex-task": "<h1 id=\"google_dataplex_task\">google_dataplex_task</h1><p>A Dataplex task represents the work that you want Dataplex to do on a schedule. It encapsulates code, parameters, and the schedule.</p><p>To get more information about Task, see:</p><ul><li><a href=\"https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.lakes.tasks\">API documentation</a></li><li>How-to Guides<ul><li><a href=\"https://cloud.google.com/dataplex/docs\">Official Documentation</a></li></ul></li></ul>", "example-usage---dataplex-task-basic": "<h2 id=\"example-usage-dataplex-task-basic\">Example Usage - Dataplex Task Basic</h2><p><br />data \"google_project\" \"project\" {</p><p>}</p><p>resource \"google_dataplex_lake\" \"example\" {<br />  name         = \"tf-test-lake%{random_suffix}\"<br />  location     = \"us-central1\"<br />  project = \"my-project-name\"<br />}</p><p>resource \"google_dataplex_task\" \"example\" {</p><pre><code>task_id      = \"tf-test-task%{random_suffix}\"location     = \"us-central1\"lake         = google_dataplex_lake.example.namedescription = \"Test Task Basic\"display_name = \"task-basic\"labels = { \"count\": \"3\" }trigger_spec  {    type = \"RECURRING\"    disabled = false    max_retries = 3    start_time = \"2023-10-02T15:01:23Z\"    schedule = \"1 * * * *\"}execution_spec {    service_account = \"${data.google_project.project.number}-compute@developer.gserviceaccount.com\"    project = \"my-project-name\"    max_job_execution_lifetime = \"100s\"    kms_key = \"234jn2kjn42k3n423\"}spark {    python_script_file = \"gs://dataproc-examples/pyspark/hello-world/hello-world.py\"}project = \"my-project-name\"</code></pre><p>}<br /></p>", "example-usage---dataplex-task-spark": "<h2 id=\"example-usage-dataplex-task-spark\">Example Usage - Dataplex Task Spark</h2><p></p>", "vpc-network": "<h1 id=\"vpc-network_1\">VPC network</h1><p>resource \"google_compute_network\" \"default\" {<br />    name                    = \"tf-test-workstation-cluster%{random_suffix}\"<br />    auto_create_subnetworks = true<br />}</p><p>data \"google_project\" \"project\" {</p><p>}</p><p>resource \"google_dataplex_lake\" \"example_notebook\" {<br />  name         = \"tf-test-lake%{random_suffix}\"<br />  location     = \"us-central1\"<br />  project = \"my-project-name\"<br />}</p><p>resource \"google_dataplex_task\" \"example_notebook\" {</p><pre><code>task_id      = \"tf-test-task%{random_suffix}\"location     = \"us-central1\"lake         = google_dataplex_lake.example_notebook.nametrigger_spec  {    type = \"RECURRING\"    schedule = \"1 * * * *\"}execution_spec {    service_account = \"${data.google_project.project.number}-compute@developer.gserviceaccount.com\"    args = {        TASK_ARGS  = \"--output_location,gs://spark-job-jars-anrajitha/task-result, --output_format, json\"    }}notebook {    notebook = \"gs://terraform-test/test-notebook.ipynb\"    infrastructure_spec  {        batch {            executors_count = 2            max_executors_count = 100        }        container_image {            image = \"test-image\"            java_jars = [\"test-java-jars.jar\"]            python_packages = [\"gs://bucket-name/my/path/to/lib.tar.gz\"]            properties = { \"name\": \"wrench\", \"mass\": \"1.3kg\", \"count\": \"3\" }        }        vpc_network  {                network_tags = [\"test-network-tag\"]                network = google_compute_network.default.id            }    }    file_uris = [\"gs://terraform-test/test.csv\"]    archive_uris = [\"gs://terraform-test/test.csv\"]}project = \"my-project-name\"</code></pre><p>}<br /></p>", "example-usage---dataplex-task-notebook": "<h2 id=\"example-usage-dataplex-task-notebook\">Example Usage - Dataplex Task Notebook</h2><p></p>", "argument-reference": "<h2 id=\"argument-reference\">Argument Reference</h2><p>The following arguments are supported:</p><ul><li><p><code>trigger_spec</code> -<br />  (Required)<br />  Configuration for the cluster<br />  Structure is <a href=\"#nested_trigger_spec\">documented below</a>.</p></li><li><p><code>execution_spec</code> -<br />  (Required)<br />  Configuration for the cluster<br />  Structure is <a href=\"#nested_execution_spec\">documented below</a>.</p></li></ul><p><a name=\"nested_trigger_spec\"></a>The <code>trigger_spec</code> block supports:</p><ul><li><p><code>type</code> -<br />  (Required)<br />  Trigger type of the user-specified Task<br />  Possible values are: <code>ON_DEMAND</code>, <code>RECURRING</code>.</p></li><li><p><code>start_time</code> -<br />  (Optional)<br />  The first run of the task will be after this time. If not specified, the task will run shortly after being submitted if ON_DEMAND and based on the schedule if RECURRING.</p></li><li><p><code>disabled</code> -<br />  (Optional)<br />  Prevent the task from executing. This does not cancel already running tasks. It is intended to temporarily disable RECURRING tasks.</p></li><li><p><code>max_retries</code> -<br />  (Optional)<br />  Number of retry attempts before aborting. Set to zero to never attempt to retry a failed task.</p></li><li><p><code>schedule</code> -<br />  (Optional)<br />  Cron schedule (https://en.wikipedia.org/wiki/Cron) for running tasks periodically. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: 'CRON_TZ=${IANA_TIME_ZONE}' or 'TZ=${IANA_TIME_ZONE}'. The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, CRON_TZ=America/New_York 1 * <em> * </em>, or TZ=America/New_York 1 * <em> * </em>. This field is required for RECURRING tasks.</p></li></ul><p><a name=\"nested_execution_spec\"></a>The <code>execution_spec</code> block supports:</p><ul><li><p><code>args</code> -<br />  (Optional)<br />  The arguments to pass to the task. The args can use placeholders of the format ${placeholder} as part of key/value string. These will be interpolated before passing the args to the driver. Currently supported placeholders: - ${taskId} - ${job_time} To pass positional args, set the key as TASK_ARGS. The value should be a comma-separated string of all the positional arguments. To use a delimiter other than comma, refer to https://cloud.google.com/sdk/gcloud/reference/topic/escaping. In case of other keys being present in the args, then TASK_ARGS will be passed as the last argument. An object containing a list of 'key': value pairs. Example: { 'name': 'wrench', 'mass': '1.3kg', 'count': '3' }.</p></li><li><p><code>service_account</code> -<br />  (Required)<br />  Service account to use to execute a task. If not provided, the default Compute service account for the project is used.</p></li><li><p><code>project</code> -<br />  (Optional)<br />  The project in which jobs are run. By default, the project containing the Lake is used. If a project is provided, the ExecutionSpec.service_account must belong to this project.</p></li><li><p><code>max_job_execution_lifetime</code> -<br />  (Optional)<br />  The maximum duration after which the job execution is expired. A duration in seconds with up to nine fractional digits, ending with 's'. Example: '3.5s'.</p></li><li><p><code>kms_key</code> -<br />  (Optional)<br />  The Cloud KMS key to use for encryption, of the form: projects/{project_number}/locations/{locationId}/keyRings/{key-ring-name}/cryptoKeys/{key-name}.</p></li></ul><hr /><ul><li><p><code>description</code> -<br />  (Optional)<br />  User-provided description of the task.</p></li><li><p><code>display_name</code> -<br />  (Optional)<br />  User friendly display name.</p></li><li><p><code>labels</code> -<br />  (Optional)<br />  User-defined labels for the task.</p></li><li><p><code>spark</code> -<br />  (Optional)<br />  A service with manual scaling runs continuously, allowing you to perform complex initialization and rely on the state of its memory over time.<br />  Structure is <a href=\"#nested_spark\">documented below</a>.</p></li><li><p><code>notebook</code> -<br />  (Optional)<br />  A service with manual scaling runs continuously, allowing you to perform complex initialization and rely on the state of its memory over time.<br />  Structure is <a href=\"#nested_notebook\">documented below</a>.</p></li><li><p><code>location</code> -<br />  (Optional)<br />  The location in which the task will be created in.</p></li><li><p><code>lake</code> -<br />  (Optional)<br />  The lake in which the task will be created in.</p></li><li><p><code>task_id</code> -<br />  (Optional)<br />  The task Id of the task.</p></li><li><p><code>project</code> - (Optional) The ID of the project in which the resource belongs.<br />    If it is not provided, the provider project is used.</p></li></ul><p><a name=\"nested_spark\"></a>The <code>spark</code> block supports:</p><ul><li><p><code>file_uris</code> -<br />  (Optional)<br />  Cloud Storage URIs of files to be placed in the working directory of each executor.</p></li><li><p><code>archive_uris</code> -<br />  (Optional)<br />  Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.</p></li><li><p><code>infrastructure_spec</code> -<br />  (Optional)<br />  Infrastructure specification for the execution.<br />  Structure is <a href=\"#nested_infrastructure_spec\">documented below</a>.</p></li><li><p><code>main_jar_file_uri</code> -<br />  (Optional)<br />  The Cloud Storage URI of the jar file that contains the main class. The execution args are passed in as a sequence of named process arguments (--key=value).</p></li><li><p><code>main_class</code> -<br />  (Optional)<br />  The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris. The execution args are passed in as a sequence of named process arguments (--key=value).</p></li><li><p><code>python_script_file</code> -<br />  (Optional)<br />  The Gcloud Storage URI of the main Python file to use as the driver. Must be a .py file. The execution args are passed in as a sequence of named process arguments (--key=value).</p></li><li><p><code>sql_script_file</code> -<br />  (Optional)<br />  A reference to a query file. This can be the Cloud Storage URI of the query file or it can the path to a SqlScript Content. The execution args are used to declare a set of script variables (set key='value';).</p></li><li><p><code>sql_script</code> -<br />  (Optional)<br />  The query text. The execution args are used to declare a set of script variables (set key='value';).</p></li></ul><p><a name=\"nested_infrastructure_spec\"></a>The <code>infrastructure_spec</code> block supports:</p><ul><li><p><code>batch</code> -<br />  (Optional)<br />  Compute resources needed for a Task when using Dataproc Serverless.<br />  Structure is <a href=\"#nested_batch\">documented below</a>.</p></li><li><p><code>container_image</code> -<br />  (Optional)<br />  Container Image Runtime Configuration.<br />  Structure is <a href=\"#nested_container_image\">documented below</a>.</p></li><li><p><code>vpc_network</code> -<br />  (Optional)<br />  Vpc network.<br />  Structure is <a href=\"#nested_vpc_network\">documented below</a>.</p></li></ul><p><a name=\"nested_batch\"></a>The <code>batch</code> block supports:</p><ul><li><p><code>executors_count</code> -<br />  (Optional)<br />  Total number of job executors. Executor Count should be between 2 and 100. [Default=2]</p></li><li><p><code>max_executors_count</code> -<br />  (Optional)<br />  Max configurable executors. If maxExecutorsCount &gt; executorsCount, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. [Default=1000]</p></li></ul><p><a name=\"nested_container_image\"></a>The <code>container_image</code> block supports:</p><ul><li><p><code>image</code> -<br />  (Optional)<br />  Container image to use.</p></li><li><p><code>java_jars</code> -<br />  (Optional)<br />  A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar</p></li><li><p><code>python_packages</code> -<br />  (Optional)<br />  A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz</p></li><li><p><code>properties</code> -<br />  (Optional)<br />  Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties.</p></li></ul><p><a name=\"nested_vpc_network\"></a>The <code>vpc_network</code> block supports:</p><ul><li><p><code>network_tags</code> -<br />  (Optional)<br />  List of network tags to apply to the job.</p></li><li><p><code>network</code> -<br />  (Optional)<br />  The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.</p></li><li><p><code>sub_network</code> -<br />  (Optional)<br />  The Cloud VPC sub-network in which the job is run.</p></li></ul><p><a name=\"nested_notebook\"></a>The <code>notebook</code> block supports:</p><ul><li><p><code>notebook</code> -<br />  (Required)<br />  Path to input notebook. This can be the Cloud Storage URI of the notebook file or the path to a Notebook Content. The execution args are accessible as environment variables (TASK_key=value).</p></li><li><p><code>infrastructure_spec</code> -<br />  (Optional)<br />  Infrastructure specification for the execution.<br />  Structure is <a href=\"#nested_infrastructure_spec\">documented below</a>.</p></li><li><p><code>file_uris</code> -<br />  (Optional)<br />  Cloud Storage URIs of files to be placed in the working directory of each executor.</p></li><li><p><code>archive_uris</code> -<br />  (Optional)<br />  Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.</p></li></ul><p><a name=\"nested_infrastructure_spec\"></a>The <code>infrastructure_spec</code> block supports:</p><ul><li><p><code>batch</code> -<br />  (Optional)<br />  Compute resources needed for a Task when using Dataproc Serverless.<br />  Structure is <a href=\"#nested_batch\">documented below</a>.</p></li><li><p><code>container_image</code> -<br />  (Optional)<br />  Container Image Runtime Configuration.<br />  Structure is <a href=\"#nested_container_image\">documented below</a>.</p></li><li><p><code>vpc_network</code> -<br />  (Optional)<br />  Vpc network.<br />  Structure is <a href=\"#nested_vpc_network\">documented below</a>.</p></li></ul><p><a name=\"nested_batch\"></a>The <code>batch</code> block supports:</p><ul><li><p><code>executors_count</code> -<br />  (Optional)<br />  Total number of job executors. Executor Count should be between 2 and 100. [Default=2]</p></li><li><p><code>max_executors_count</code> -<br />  (Optional)<br />  Max configurable executors. If maxExecutorsCount &gt; executorsCount, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. [Default=1000]</p></li></ul><p><a name=\"nested_container_image\"></a>The <code>container_image</code> block supports:</p><ul><li><p><code>image</code> -<br />  (Optional)<br />  Container image to use.</p></li><li><p><code>java_jars</code> -<br />  (Optional)<br />  A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar</p></li><li><p><code>python_packages</code> -<br />  (Optional)<br />  A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz</p></li><li><p><code>properties</code> -<br />  (Optional)<br />  Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties.</p></li></ul><p><a name=\"nested_vpc_network\"></a>The <code>vpc_network</code> block supports:</p><ul><li><p><code>network_tags</code> -<br />  (Optional)<br />  List of network tags to apply to the job.</p></li><li><p><code>network</code> -<br />  (Optional)<br />  The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.</p></li><li><p><code>sub_network</code> -<br />  (Optional)<br />  The Cloud VPC sub-network in which the job is run.</p></li></ul>", "attributes-reference": "<h2 id=\"attributes-reference\">Attributes Reference</h2><p>In addition to the arguments listed above, the following computed attributes are exported:</p><ul><li><p><code>id</code> - an identifier for the resource with format <code>projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}</code></p></li><li><p><code>name</code> -<br />  The relative resource name of the task, of the form: projects/{project_number}/locations/{locationId}/lakes/{lakeId}/ tasks/{name}.</p></li><li><p><code>uid</code> -<br />  System generated globally unique ID for the task. This ID will be different if the task is deleted and re-created with the same name.</p></li><li><p><code>create_time</code> -<br />  The time when the task was created.</p></li><li><p><code>update_time</code> -<br />  The time when the task was last updated.</p></li><li><p><code>state</code> -<br />  Current state of the task.</p></li><li><p><code>execution_status</code> -<br />  Configuration for the cluster<br />  Structure is <a href=\"#nested_execution_status\">documented below</a>.</p></li></ul><p><a name=\"nested_execution_status\"></a>The <code>execution_status</code> block contains:</p><ul><li><p><code>update_time</code> -<br />  (Output)<br />  Last update time of the status.</p></li><li><p><code>latest_job</code> -<br />  (Output)<br />  latest job execution.<br />  Structure is <a href=\"#nested_latest_job\">documented below</a>.</p></li></ul><p><a name=\"nested_latest_job\"></a>The <code>latest_job</code> block contains:</p><ul><li><p><code>name</code> -<br />  (Output)<br />  The relative resource name of the job, of the form: projects/{project_number}/locations/{locationId}/lakes/{lakeId}/tasks/{taskId}/jobs/{jobId}.</p></li><li><p><code>uid</code> -<br />  (Output)<br />  System generated globally unique ID for the job.</p></li><li><p><code>start_time</code> -<br />  (Output)<br />  The time when the job was started.</p></li><li><p><code>end_time</code> -<br />  (Output)<br />  The time when the job ended.</p></li><li><p><code>state</code> -<br />  (Output)<br />  Execution state for the job.</p></li><li><p><code>retry_count</code> -<br />  (Output)<br />  The number of times the job has been retried (excluding the initial attempt).</p></li><li><p><code>service</code> -<br />  (Output)<br />  The underlying service running a job.</p></li><li><p><code>service_job</code> -<br />  (Output)<br />  The full resource name for the job run under a particular service.</p></li><li><p><code>message</code> -<br />  (Output)<br />  Additional information about the current state.</p></li></ul>", "timeouts": "<h2 id=\"timeouts\">Timeouts</h2><p>This resource provides the following<br /><a href=\"https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts\">Timeouts</a> configuration options:</p><ul><li><code>create</code> - Default is 5 minutes.</li><li><code>update</code> - Default is 5 minutes.</li><li><code>delete</code> - Default is 5 minutes.</li></ul>", "import": "<h2 id=\"import\">Import</h2><p>Task can be imported using any of these accepted formats:</p><p><code>$ terraform import google_dataplex_task.default projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}$ terraform import google_dataplex_task.default {{project}}/{{location}}/{{lake}}/{{task_id}}$ terraform import google_dataplex_task.default {{location}}/{{lake}}/{{task_id}}</code></p>", "user-project-overrides": "<h2 id=\"user-project-overrides\">User Project Overrides</h2><p>This resource supports <a href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#user_project_override\">User Project Overrides</a>.</p>", "description": "<h1 id=\"google_dataplex_task\">google_dataplex_task</h1><p>A Dataplex task represents the work that you want Dataplex to do on a schedule. It encapsulates code, parameters, and the schedule.</p><p>To get more information about Task, see:</p><ul><li><a href=\"https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.lakes.tasks\">API documentation</a></li><li>How-to Guides<ul><li><a href=\"https://cloud.google.com/dataplex/docs\">Official Documentation</a></li></ul></li></ul>"}