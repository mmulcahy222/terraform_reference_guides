{"google-dataproc-cluster": "<h1 id=\"google_dataproc_cluster\">google_dataproc_cluster</h1><p>Manages a Cloud Dataproc cluster resource within GCP.</p><ul><li><a href=\"https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.clusters\">API documentation</a></li><li>How-to Guides<ul><li><a href=\"https://cloud.google.com/dataproc/docs\">Official Documentation</a></li></ul></li></ul><p>!&gt; <strong>Warning:</strong> Due to limitations of the API, all arguments except<br /><code>labels</code>,<code>cluster_config.worker_config.num_instances</code> and <code>cluster_config.preemptible_worker_config.num_instances</code> are non-updatable. Changing others will cause recreation of the<br />whole cluster!</p>", "example-usage---basic": "<h2 id=\"example-usage-basic\">Example Usage - Basic</h2><p><code>hclresource \"google_dataproc_cluster\" \"simplecluster\" {  name   = \"simplecluster\"  region = \"us-central1\"}</code></p>", "example-usage---advanced": "<h2 id=\"example-usage-advanced\">Example Usage - Advanced</h2><p><br />resource \"google_service_account\" \"default\" {<br />  account_id   = \"service-account-id\"<br />  display_name = \"Service Account\"<br />}</p><p>resource \"google_dataproc_cluster\" \"mycluster\" {<br />  name     = \"mycluster\"<br />  region   = \"us-central1\"<br />  graceful_decommission_timeout = \"120s\"<br />  labels = {<br />    foo = \"bar\"<br />  }</p><p>cluster_config {<br />    staging_bucket = \"dataproc-staging-bucket\"</p><pre><code>master_config {  num_instances = 1  machine_type  = \"e2-medium\"  disk_config {    boot_disk_type    = \"pd-ssd\"    boot_disk_size_gb = 30  }}worker_config {  num_instances    = 2  machine_type     = \"e2-medium\"  min_cpu_platform = \"Intel Skylake\"  disk_config {    boot_disk_size_gb = 30    num_local_ssds    = 1  }}preemptible_worker_config {  num_instances = 0}# Override or set some custom propertiessoftware_config {  image_version = \"2.0.35-debian10\"  override_properties = {    \"dataproc:dataproc.allow.zero.workers\" = \"true\"  }}gce_cluster_config {  tags = [\"foo\", \"bar\"]  # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.  service_account = google_service_account.default.email  service_account_scopes = [    \"cloud-platform\"  ]}# You can define multiple initialization_action blocksinitialization_action {  script      = \"gs://dataproc-initialization-actions/stackdriver/stackdriver.sh\"  timeout_sec = 500}</code></pre><p>}<br />}<br /></p>", "example-usage---using-a-gpu-accelerator": "<h2 id=\"example-usage-using-a-gpu-accelerator\">Example Usage - Using a GPU accelerator</h2><p><br />resource \"google_dataproc_cluster\" \"accelerated_cluster\" {<br />  name   = \"my-cluster-with-gpu\"<br />  region = \"us-central1\"</p><p>cluster_config {<br />    gce_cluster_config {<br />      zone = \"us-central1-a\"<br />    }</p><pre><code>master_config {  accelerators {    accelerator_type  = \"nvidia-tesla-k80\"    accelerator_count = \"1\"  }}</code></pre><p>}<br />}<br /></p>", "argument-reference": "<h2 id=\"argument-reference\">Argument Reference</h2><ul><li><code>name</code> - (Required) The name of the cluster, unique within the project and<br />    zone.</li></ul><hr /><ul><li><p><code>project</code> - (Optional) The ID of the project in which the <code>cluster</code> will exist. If it<br />    is not provided, the provider project is used.</p></li><li><p><code>region</code> - (Optional) The region in which the cluster and associated nodes will be created in.<br />   Defaults to <code>global</code>.</p></li><li><p><code>labels</code> - (Optional, Computed) The list of labels (key/value pairs) to be applied to<br />   instances in the cluster. GCP generates some itself including <code>goog-dataproc-cluster-name</code><br />   which is the name of the cluster.</p></li><li><p><code>virtual_cluster_config</code> - (Optional) Allows you to configure a virtual Dataproc on GKE cluster.<br />   Structure <a href=\"#nested_virtual_cluster_config\">defined below</a>.</p></li><li><p><code>cluster_config</code> - (Optional) Allows you to configure various aspects of the cluster.<br />   Structure <a href=\"#nested_cluster_config\">defined below</a>.</p></li><li><p><code>graceful_decommission_timeout</code> - (Optional) Allows graceful decomissioning when you change the number of worker nodes directly through a terraform apply.<br />      Does not affect auto scaling decomissioning from an autoscaling policy.<br />      Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress.<br />      Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs).<br />      Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of<br /><a href=\"https://developers.google.com/protocol-buffers/docs/proto3#json\">Duration</a>).<br />      Only supported on Dataproc image versions 1.2 and higher.<br />      For more context see the <a href=\"https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.clusters/patch#query-parameters\">docs</a></p></li></ul><hr /><p><a name=\"nested_virtual_cluster_config\"></a>The <code>virtual_cluster_config</code> block supports:</p><p><code>hcl    virtual_cluster_config {        auxiliary_services_config { ... }        kubernetes_cluster_config { ... }    }</code></p><ul><li><p><code>staging_bucket</code> - (Optional) The Cloud Storage staging bucket used to stage files,<br />   such as Hadoop jars, between client machines and the cluster.<br />   Note: If you don't explicitly specify a <code>staging_bucket</code><br />   then GCP will auto create / assign one for you. However, you are not guaranteed<br />   an auto generated bucket which is solely dedicated to your cluster; it may be shared<br />   with other clusters in the same region/zone also choosing to use the auto generation<br />   option.</p></li><li><p><code>auxiliary_services_config</code> (Optional) Configuration of auxiliary services used by this cluster. <br />   Structure <a href=\"#nested_auxiliary_services_config\">defined below</a>.</p></li><li><p><code>kubernetes_cluster_config</code> (Required) The configuration for running the Dataproc cluster on Kubernetes.<br />   Structure <a href=\"#nested_kubernetes_cluster_config\">defined below</a>.</p></li></ul><hr /><p><a name=\"nested_auxiliary_services_config\"></a>The <code>auxiliary_services_config</code> block supports:</p><p><br />    virtual_cluster_config {<br />      auxiliary_services_config {<br />        metastore_config {<br />          dataproc_metastore_service = google_dataproc_metastore_service.metastore_service.id<br />        }</p><pre><code>    spark_history_server_config {      dataproc_cluster = google_dataproc_cluster.dataproc_cluster.id    }  }}</code></pre><p></p><ul><li><p><code>metastore_config</code> (Optional) The Hive Metastore configuration for this workload. </p></li><li><p><code>dataproc_metastore_service</code> (Required) Resource name of an existing Dataproc Metastore service.</p></li><li><p><code>spark_history_server_config</code> (Optional) The Spark History Server configuration for the workload.</p></li><li><p><code>dataproc_cluster</code> (Optional) Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.</p></li></ul><hr /><p><a name=\"nested_kubernetes_cluster_config\"></a>The <code>kubernetes_cluster_config</code> block supports:</p><p><br />    virtual_cluster_config {<br />      kubernetes_cluster_config {<br />        kubernetes_namespace = \"foobar\"</p><pre><code>    kubernetes_software_config {      component_version = {        \"SPARK\" : \"3.1-dataproc-7\"      }      properties = {        \"spark:spark.eventLog.enabled\": \"true\"      }    }    gke_cluster_config {      gke_cluster_target = google_container_cluster.primary.id      node_pool_target {        node_pool = \"dpgke\"        roles = [\"DEFAULT\"]        node_pool_config {          autoscaling {            min_node_count = 1            max_node_count = 6          }          config {            machine_type      = \"n1-standard-4\"            preemptible       = true            local_ssd_count   = 1            min_cpu_platform  = \"Intel Sandy Bridge\"          }          locations = [\"us-central1-c\"]        }      }    }  }}</code></pre><p></p><ul><li><p><code>kubernetes_namespace</code> (Optional) A namespace within the Kubernetes cluster to deploy into. <br />   If this namespace does not exist, it is created. <br />   If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. <br />   If not specified, the name of the Dataproc Cluster is used.</p></li><li><p><code>kubernetes_software_config</code> (Required) The software configuration for this Dataproc cluster running on Kubernetes.</p></li><li><p><code>component_version</code> (Required) The components that should be installed in this Dataproc cluster. The key must be a string from the <br />     KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.</p><ul><li><strong>NOTE</strong> : <code>component_version[SPARK]</code> is mandatory to set, or the creation of the cluster will fail.</li></ul></li><li><p><code>properties</code> (Optional) The properties to set on daemon config files. Property keys are specified in prefix:property format, <br />     for example spark:spark.kubernetes.container.image.</p></li><li><p><code>gke_cluster_config</code> (Required) The configuration for running the Dataproc cluster on GKE.</p></li><li><p><code>gke_cluster_target</code> (Optional) A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster <br />     (the GKE cluster can be zonal or regional)</p></li><li><p><code>node_pool_target</code> (Optional) GKE node pools where workloads will be scheduled. At least one node pool must be assigned the <code>DEFAULT</code> <br />     GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a <code>DEFAULT</code> GkeNodePoolTarget. <br />     Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.</p><ul><li><p><code>node_pool</code> (Required) The target GKE node pool.</p></li><li><p><code>roles</code> (Required) The roles associated with the GKE node pool. <br />   One of <code>\"DEFAULT\"</code>, <code>\"CONTROLLER\"</code>, <code>\"SPARK_DRIVER\"</code> or <code>\"SPARK_EXECUTOR\"</code>.</p></li><li><p><code>node_pool_config</code> (Input only) The configuration for the GKE node pool. <br />   If specified, Dataproc attempts to create a node pool with the specified shape. <br />   If one with the same name already exists, it is verified against all specified fields. <br />   If a field differs, the virtual cluster creation will fail.</p></li><li><p><code>autoscaling</code> (Optional) The autoscaler configuration for this node pool. <br />     The autoscaler is enabled only when a valid configuration is present.</p><ul><li><p><code>min_node_count</code> (Optional) The minimum number of nodes in the node pool. Must be &gt;= 0 and &lt;= maxNodeCount.</p></li><li><p><code>max_node_count</code> (Optional) The maximum number of nodes in the node pool. Must be &gt;= minNodeCount, and must be &gt; 0.</p></li></ul></li><li><p><code>config</code> (Optional) The node pool configuration.</p><ul><li><p><code>machine_type</code> (Optional) The name of a Compute Engine machine type.</p></li><li><p><code>local_ssd_count</code> (Optional) The number of local SSD disks to attach to the node, <br />   which is limited by the maximum number of disks allowable per zone.</p></li><li><p><code>preemptible</code> (Optional) Whether the nodes are created as preemptible VM instances. <br />   Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the <br />   CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).</p></li><li><p><code>min_cpu_platform</code> (Optional) Minimum CPU platform to be used by this instance. <br />   The instance may be scheduled on the specified or a newer CPU platform. <br />   Specify the friendly names of CPU platforms, such as \"Intel Haswell\" or \"Intel Sandy Bridge\".</p></li><li><p><code>spot</code> (Optional) Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.</p></li></ul></li><li><p><code>locations</code> (Optional) The list of Compute Engine zones where node pool nodes associated <br />     with a Dataproc on GKE virtual cluster will be located.</p></li></ul><hr /></li></ul><p><a name=\"nested_cluster_config\"></a>The <code>cluster_config</code> block supports:</p><p><br />    cluster_config {<br />        gce_cluster_config        { ... }<br />        master_config             { ... }<br />        worker_config             { ... }<br />        preemptible_worker_config { ... }<br />        software_config           { ... }</p><pre><code>    # You can define multiple initialization_action blocks    initialization_action     { ... }    encryption_config         { ... }    endpoint_config           { ... }    metastore_config          { ... }}</code></pre><p></p><ul><li><p><code>staging_bucket</code> - (Optional) The Cloud Storage staging bucket used to stage files,<br />   such as Hadoop jars, between client machines and the cluster.<br />   Note: If you don't explicitly specify a <code>staging_bucket</code><br />   then GCP will auto create / assign one for you. However, you are not guaranteed<br />   an auto generated bucket which is solely dedicated to your cluster; it may be shared<br />   with other clusters in the same region/zone also choosing to use the auto generation<br />   option.</p></li><li><p><code>temp_bucket</code> - (Optional) The Cloud Storage temp bucket used to store ephemeral cluster<br />   and jobs data, such as Spark and MapReduce history files.<br />   Note: If you don't explicitly specify a <code>temp_bucket</code> then GCP will auto create / assign one for you.</p></li><li><p><code>gce_cluster_config</code> (Optional) Common config settings for resources of Google Compute Engine cluster<br />   instances, applicable to all instances in the cluster. Structure <a href=\"#nested_gce_cluster_config\">defined below</a>.</p></li><li><p><code>master_config</code> (Optional) The Google Compute Engine config settings for the master instances<br />   in a cluster. Structure <a href=\"#nested_master_config\">defined below</a>.</p></li><li><p><code>worker_config</code> (Optional) The Google Compute Engine config settings for the worker instances<br />   in a cluster. Structure <a href=\"#nested_worker_config\">defined below</a>.</p></li><li><p><code>preemptible_worker_config</code> (Optional) The Google Compute Engine config settings for the additional<br />   instances in a cluster. Structure <a href=\"#nested_preemptible_worker_config\">defined below</a>.</p></li><li><p><strong>NOTE</strong> : <code>preemptible_worker_config</code> is<br />   an alias for the api's <a href=\"https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig#InstanceGroupConfig\">secondaryWorkerConfig</a>. The name doesn't necessarily mean it is preemptible and is named as<br />   such for legacy/compatibility reasons.</p></li><li><p><code>software_config</code> (Optional) The config settings for software inside the cluster.<br />   Structure <a href=\"#nested_software_config\">defined below</a>.</p></li><li><p><code>security_config</code> (Optional) Security related configuration. Structure <a href=\"#nested_security_config\">defined below</a>.</p></li><li><p><code>autoscaling_config</code> (Optional)  The autoscaling policy config associated with the cluster.<br />   Note that once set, if <code>autoscaling_config</code> is the only field set in <code>cluster_config</code>, it can<br />   only be removed by setting <code>policy_uri = \"\"</code>, rather than removing the whole block.<br />   Structure <a href=\"#nested_autoscaling_config\">defined below</a>.</p></li><li><p><code>initialization_action</code> (Optional) Commands to execute on each node after config is completed.<br />   You can specify multiple versions of these. Structure <a href=\"#nested_initialization_action\">defined below</a>.</p></li><li><p><code>encryption_config</code> (Optional) The Customer managed encryption keys settings for the cluster.<br />   Structure <a href=\"#nested_encryption_config\">defined below</a>.</p></li><li><p><code>lifecycle_config</code> (Optional) The settings for auto deletion cluster schedule.<br />   Structure <a href=\"#nested_lifecycle_config\">defined below</a>.</p></li><li><p><code>endpoint_config</code> (Optional) The config settings for port access on the cluster.<br />   Structure <a href=\"#nested_endpoint_config\">defined below</a>.</p></li><li><p><code>dataproc_metric_config</code> (Optional) The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.<br />   Structure <a href=\"#nested_dataproc_metric_config\">defined below</a>.</p></li><li><p><code>metastore_config</code> (Optional) The config setting for metastore service with the cluster.<br />   Structure <a href=\"#nested_metastore_config\">defined below</a>.</p></li></ul><hr /><p><a name=\"nested_gce_cluster_config\"></a>The <code>cluster_config.gce_cluster_config</code> block supports:</p><p><br />  cluster_config {<br />    gce_cluster_config {<br />      zone = \"us-central1-a\"</p><pre><code>  # One of the below to hook into a custom network / subnetwork  network    = google_compute_network.dataproc_network.name  subnetwork = google_compute_network.dataproc_subnetwork.name  tags = [\"foo\", \"bar\"]}</code></pre><p>}<br /></p><ul><li><p><code>zone</code> - (Optional, Computed) The GCP zone where your data is stored and used (i.e. where<br />    the master and the worker nodes will be created in). If <code>region</code> is set to 'global' (default)<br />    then <code>zone</code> is mandatory, otherwise GCP is able to make use of <a href=\"https://cloud.google.com/dataproc/docs/concepts/auto-zone\">Auto Zone Placement</a><br />    to determine this automatically for you.<br />    Note: This setting additionally determines and restricts<br />    which computing resources are available for use with other configs such as<br /><code>cluster_config.master_config.machine_type</code> and <code>cluster_config.worker_config.machine_type</code>.</p></li><li><p><code>network</code> - (Optional, Computed) The name or self_link of the Google Compute Engine<br />    network to the cluster will be part of. Conflicts with <code>subnetwork</code>.<br />    If neither is specified, this defaults to the \"default\" network.</p></li><li><p><code>subnetwork</code> - (Optional) The name or self_link of the Google Compute Engine<br />   subnetwork the cluster will be part of. Conflicts with <code>network</code>.</p></li><li><p><code>service_account</code> - (Optional) The service account to be used by the Node VMs.<br />    If not specified, the \"default\" service account is used.</p></li><li><p><code>service_account_scopes</code> - (Optional, Computed) The set of Google API scopes<br />    to be made available on all of the node VMs under the <code>service_account</code><br />    specified. Both OAuth2 URLs and gcloud<br />    short names are supported. To allow full access to all Cloud APIs, use the<br /><code>cloud-platform</code> scope. See a complete list of scopes <a href=\"https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes\">here</a>.</p></li><li><p><code>tags</code> - (Optional) The list of instance tags applied to instances in the cluster.<br />   Tags are used to identify valid sources or targets for network firewalls.</p></li><li><p><code>internal_ip_only</code> - (Optional) By default, clusters are not restricted to internal IP addresses,<br />   and will have ephemeral external IP addresses assigned to each instance. If set to true, all<br />   instances in the cluster will only have internal IP addresses. Note: Private Google Access<br />   (also known as <code>privateIpGoogleAccess</code>) must be enabled on the subnetwork that the cluster<br />   will be launched in.</p></li><li><p><code>metadata</code> - (Optional) A map of the Compute Engine metadata entries to add to all instances<br />   (see <a href=\"https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata\">Project and instance metadata</a>).</p></li><li><p><code>reservation_affinity</code> - (Optional) Reservation Affinity for consuming zonal reservation.</p><ul><li><code>consume_reservation_type</code> - (Optional) Corresponds to the type of reservation consumption.</li><li><code>key</code> - (Optional) Corresponds to the label key of reservation resource.</li><li><code>values</code> - (Optional) Corresponds to the label values of reservation resource.</li></ul></li><li><p><code>node_group_affinity</code> - (Optional) Node Group Affinity for sole-tenant clusters.</p><ul><li><code>node_group_uri</code> - (Required) The URI of a sole-tenant node group resource that the cluster will be created on.</li></ul></li><li><p><code>shielded_instance_config</code> (Optional) Shielded Instance Config for clusters using <a href=\"https://cloud.google.com/security/shielded-cloud/shielded-vm\">Compute Engine Shielded VMs</a>.</p></li></ul><hr /><p>The <code>cluster_config.gce_cluster_config.shielded_instance_config</code> block supports:</p><p><code>hclcluster_config{  gce_cluster_config{    shielded_instance_config{      enable_secure_boot          = true      enable_vtpm                 = true      enable_integrity_monitoring = true    }  }}</code></p><ul><li><p><code>enable_secure_boot</code> - (Optional) Defines whether instances have Secure Boot enabled.</p></li><li><p><code>enable_vtpm</code> - (Optional) Defines whether instances have the <a href=\"https://cloud.google.com/security/shielded-cloud/shielded-vm#vtpm\">vTPM</a> enabled.</p></li><li><p><code>enable_integrity_monitoring</code> - (Optional) Defines whether instances have integrity monitoring enabled.</p></li></ul><hr /><p><a name=\"nested_master_config\"></a>The <code>cluster_config.master_config</code> block supports:</p><p><br />cluster_config {<br />  master_config {<br />    num_instances    = 1<br />    machine_type     = \"e2-medium\"<br />    min_cpu_platform = \"Intel Skylake\"</p><pre><code>disk_config {  boot_disk_type    = \"pd-ssd\"  boot_disk_size_gb = 30  num_local_ssds    = 1}</code></pre><p>}<br />}<br /></p><ul><li><p><code>num_instances</code>- (Optional, Computed) Specifies the number of master nodes to create.<br />   If not specified, GCP will default to a predetermined computed value (currently 1).</p></li><li><p><code>machine_type</code> - (Optional, Computed) The name of a Google Compute Engine machine type<br />   to create for the master. If not specified, GCP will default to a predetermined<br />   computed value (currently <code>n1-standard-4</code>).</p></li><li><p><code>min_cpu_platform</code> - (Optional, Computed) The name of a minimum generation of CPU family<br />   for the master. If not specified, GCP will default to a predetermined computed value<br />   for each zone. See <a href=\"https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform\">the guide</a><br />   for details about which CPU families are available (and defaulted) for each zone.</p></li><li><p><code>image_uri</code> (Optional) The URI for the image to use for this worker.  See <a href=\"https://cloud.google.com/dataproc/docs/guides/dataproc-images\">the guide</a><br />    for more information.</p></li><li><p><code>disk_config</code> (Optional) Disk Config</p><ul><li><p><code>boot_disk_type</code> - (Optional) The disk type of the primary disk attached to each node.<br />One of <code>\"pd-ssd\"</code> or <code>\"pd-standard\"</code>. Defaults to <code>\"pd-standard\"</code>.</p></li><li><p><code>boot_disk_size_gb</code> - (Optional, Computed) Size of the primary disk attached to each node, specified<br />in GB. The primary disk contains the boot volume and system libraries, and the<br />smallest allowed disk size is 10GB. GCP will default to a predetermined<br />computed value if not set (currently 500GB). Note: If SSDs are not<br />attached, it also contains the HDFS data blocks and Hadoop working directories.</p></li><li><p><code>num_local_ssds</code> - (Optional) The amount of local SSD disks that will be<br />attached to each master cluster node. Defaults to 0.</p></li></ul></li><li><p><code>accelerators</code> (Optional) The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.</p><ul><li><p><code>accelerator_type</code> - (Required) The short name of the accelerator type to expose to this instance. For example, <code>nvidia-tesla-k80</code>.</p></li><li><p><code>accelerator_count</code> - (Required) The number of the accelerator cards of this type exposed to this instance. Often restricted to one of <code>1</code>, <code>2</code>, <code>4</code>, or <code>8</code>.</p></li></ul></li></ul><p>~&gt; The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select<br />zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check <a href=\"https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list\">accelerator availability by zone</a><br />if you are trying to use accelerators in a given zone.</p><hr /><p><a name=\"nested_worker_config\"></a>The <code>cluster_config.worker_config</code> block supports:</p><p><br />cluster_config {<br />  worker_config {<br />    num_instances    = 3<br />    machine_type     = \"e2-medium\"<br />    min_cpu_platform = \"Intel Skylake\"</p><pre><code>disk_config {  boot_disk_type    = \"pd-standard\"  boot_disk_size_gb = 30  num_local_ssds    = 1}</code></pre><p>}<br />}<br /></p><ul><li><p><code>num_instances</code>- (Optional, Computed) Specifies the number of worker nodes to create.<br />   If not specified, GCP will default to a predetermined computed value (currently 2).<br />   There is currently a beta feature which allows you to run a<br /><a href=\"https://cloud.google.com/dataproc/docs/concepts/single-node-clusters\">Single Node Cluster</a>.<br />   In order to take advantage of this you need to set<br /><code>\"dataproc:dataproc.allow.zero.workers\" = \"true\"</code> in<br /><code>cluster_config.software_config.properties</code></p></li><li><p><code>machine_type</code> - (Optional, Computed) The name of a Google Compute Engine machine type<br />   to create for the worker nodes. If not specified, GCP will default to a predetermined<br />   computed value (currently <code>n1-standard-4</code>).</p></li><li><p><code>min_cpu_platform</code> - (Optional, Computed) The name of a minimum generation of CPU family<br />   for the master. If not specified, GCP will default to a predetermined computed value<br />   for each zone. See <a href=\"https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform\">the guide</a><br />   for details about which CPU families are available (and defaulted) for each zone.</p></li><li><p><code>disk_config</code> (Optional) Disk Config</p><ul><li><p><code>boot_disk_type</code> - (Optional) The disk type of the primary disk attached to each node.<br />One of <code>\"pd-ssd\"</code> or <code>\"pd-standard\"</code>. Defaults to <code>\"pd-standard\"</code>.</p></li><li><p><code>boot_disk_size_gb</code> - (Optional, Computed) Size of the primary disk attached to each worker node, specified<br />in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined<br />computed value if not set (currently 500GB). Note: If SSDs are not<br />attached, it also contains the HDFS data blocks and Hadoop working directories.</p></li><li><p><code>num_local_ssds</code> - (Optional) The amount of local SSD disks that will be<br />attached to each worker cluster node. Defaults to 0.</p></li></ul></li><li><p><code>image_uri</code> (Optional) The URI for the image to use for this worker.  See <a href=\"https://cloud.google.com/dataproc/docs/guides/dataproc-images\">the guide</a><br />    for more information.</p></li><li><p><code>accelerators</code> (Optional) The Compute Engine accelerator configuration for these instances. Can be specified multiple times.</p><ul><li><p><code>accelerator_type</code> - (Required) The short name of the accelerator type to expose to this instance. For example, <code>nvidia-tesla-k80</code>.</p></li><li><p><code>accelerator_count</code> - (Required) The number of the accelerator cards of this type exposed to this instance. Often restricted to one of <code>1</code>, <code>2</code>, <code>4</code>, or <code>8</code>.</p></li></ul></li></ul><p>~&gt; The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select<br />zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check <a href=\"https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list\">accelerator availability by zone</a><br />if you are trying to use accelerators in a given zone.</p><hr /><p><a name=\"nested_preemptible_worker_config\"></a>The <code>cluster_config.preemptible_worker_config</code> block supports:</p><p><br />cluster_config {<br />  preemptible_worker_config {<br />    num_instances = 1</p><pre><code>disk_config {  boot_disk_type    = \"pd-standard\"  boot_disk_size_gb = 30  num_local_ssds    = 1}</code></pre><p>}<br />}<br /></p><p>Note: Unlike <code>worker_config</code>, you cannot set the <code>machine_type</code> value directly. This<br />will be set for you based on whatever was set for the <code>worker_config.machine_type</code> value.</p><ul><li><p><code>num_instances</code>- (Optional) Specifies the number of preemptible nodes to create.<br />   Defaults to 0.</p></li><li><p><code>preemptibility</code>- (Optional) Specifies the preemptibility of the secondary workers. The default value is <code>PREEMPTIBLE</code><br />  Accepted values are:</p></li><li>PREEMPTIBILITY_UNSPECIFIED</li><li>NON_PREEMPTIBLE</li><li>PREEMPTIBLE</li><li><p>SPOT</p></li><li><p><code>disk_config</code> (Optional) Disk Config</p><ul><li><p><code>boot_disk_type</code> - (Optional) The disk type of the primary disk attached to each preemptible worker node.<br />One of <code>\"pd-ssd\"</code> or <code>\"pd-standard\"</code>. Defaults to <code>\"pd-standard\"</code>.</p></li><li><p><code>boot_disk_size_gb</code> - (Optional, Computed) Size of the primary disk attached to each preemptible worker node, specified<br />in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined<br />computed value if not set (currently 500GB). Note: If SSDs are not<br />attached, it also contains the HDFS data blocks and Hadoop working directories.</p></li><li><p><code>num_local_ssds</code> - (Optional) The amount of local SSD disks that will be<br />attached to each preemptible worker node. Defaults to 0.</p></li></ul></li></ul><hr /><p><a name=\"nested_software_config\"></a>The <code>cluster_config.software_config</code> block supports:</p><p><br />cluster_config {<br />  # Override or set some custom properties<br />  software_config {<br />    image_version = \"2.0.35-debian10\"</p><pre><code>override_properties = {  \"dataproc:dataproc.allow.zero.workers\" = \"true\"}</code></pre><p>}<br />}<br /></p><ul><li><p><code>image_version</code> - (Optional, Computed) The Cloud Dataproc image version to use<br />   for the cluster - this controls the sets of software versions<br />   installed onto the nodes when you create clusters. If not specified, defaults to the<br />   latest version. For a list of valid versions see<br /><a href=\"https://cloud.google.com/dataproc/docs/concepts/dataproc-versions\">Cloud Dataproc versions</a></p></li><li><p><code>override_properties</code> - (Optional) A list of override and additional properties (key/value pairs)<br />   used to modify various aspects of the common configuration files used when creating<br />   a cluster. For a list of valid properties please see<br /><a href=\"https://cloud.google.com/dataproc/docs/concepts/cluster-properties\">Cluster properties</a></p></li><li><p><code>optional_components</code> - (Optional) The set of optional components to activate on the cluster. See <a href=\"https://cloud.google.com/dataproc/docs/concepts/components/overview#available_optional_components\">Available Optional Components</a>.</p></li></ul><hr /><p><a name=\"nested_security_config\"></a>The <code>cluster_config.security_config</code> block supports:</p><p><code>hclcluster_config {  # Override or set some custom properties  security_config {    kerberos_config {      kms_key_uri = \"projects/projectId/locations/locationId/keyRings/keyRingId/cryptoKeys/keyId\"      root_principal_password_uri = \"bucketId/o/objectId\"    }  }}</code></p><ul><li><p><code>kerberos_config</code> (Required) Kerberos Configuration</p><ul><li><p><code>cross_realm_trust_admin_server</code> - (Optional) The admin server (IP or hostname) for the<br />   remote trusted realm in a cross realm trust relationship.</p></li><li><p><code>cross_realm_trust_kdc</code> - (Optional) The KDC (IP or hostname) for the<br />   remote trusted realm in a cross realm trust relationship.</p></li><li><p><code>cross_realm_trust_realm</code> - (Optional) The remote realm the Dataproc on-cluster KDC will<br />   trust, should the user enable cross realm trust.</p></li><li><p><code>cross_realm_trust_shared_password_uri</code> - (Optional) The Cloud Storage URI of a KMS<br />   encrypted file containing the shared password between the on-cluster Kerberos realm<br />   and the remote trusted realm, in a cross realm trust relationship.</p></li><li><p><code>enable_kerberos</code> - (Optional) Flag to indicate whether to Kerberize the cluster.</p></li><li><p><code>kdc_db_key_uri</code> - (Optional) The Cloud Storage URI of a KMS encrypted file containing<br />   the master key of the KDC database.</p></li><li><p><code>key_password_uri</code> - (Optional) The Cloud Storage URI of a KMS encrypted file containing<br />   the password to the user provided key. For the self-signed certificate, this password<br />   is generated by Dataproc.</p></li><li><p><code>keystore_uri</code> - (Optional) The Cloud Storage URI of the keystore file used for SSL encryption.<br />   If not provided, Dataproc will provide a self-signed certificate.</p></li><li><p><code>keystore_password_uri</code> - (Optional) The Cloud Storage URI of a KMS encrypted file containing<br />   the password to the user provided keystore. For the self-signed certificated, the password<br />   is generated by Dataproc.</p></li><li><p><code>kms_key_uri</code> - (Required) The URI of the KMS key used to encrypt various sensitive files.</p></li><li><p><code>realm</code> - (Optional) The name of the on-cluster Kerberos realm. If not specified, the<br />   uppercased domain of hostnames will be the realm.</p></li><li><p><code>root_principal_password_uri</code> - (Required) The Cloud Storage URI of a KMS encrypted file<br />   containing the root principal password.</p></li><li><p><code>tgt_lifetime_hours</code> - (Optional) The lifetime of the ticket granting ticket, in hours.</p></li><li><p><code>truststore_password_uri</code> - (Optional) The Cloud Storage URI of a KMS encrypted file<br />   containing the password to the user provided truststore. For the self-signed<br />   certificate, this password is generated by Dataproc.</p></li><li><p><code>truststore_uri</code> - (Optional) The Cloud Storage URI of the truststore file used for<br />   SSL encryption. If not provided, Dataproc will provide a self-signed certificate.</p></li></ul></li></ul><hr /><p><a name=\"nested_autoscaling_config\"></a>The <code>cluster_config.autoscaling_config</code> block supports:</p><p><code>hclcluster_config {  # Override or set some custom properties  autoscaling_config {    policy_uri = \"projects/projectId/locations/region/autoscalingPolicies/policyId\"  }}</code></p><ul><li><code>policy_uri</code> - (Required) The autoscaling policy used by the cluster.</li></ul><p>Only resource names including projectid and location (region) are valid. Examples:</p><p><code>https://www.googleapis.com/compute/v1/projects/[projectId]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]</code><br /><code>projects/[projectId]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]</code><br />Note that the policy must be in the same project and Cloud Dataproc region.</p><hr /><p><a name=\"nested_initialization_action\"></a>The <code>initialization_action</code> block (Optional) can be specified multiple times and supports:</p><p><code>hclcluster_config {  # You can define multiple initialization_action blocks  initialization_action {    script      = \"gs://dataproc-initialization-actions/stackdriver/stackdriver.sh\"    timeout_sec = 500  }}</code></p><ul><li><p><code>script</code>- (Required) The script to be executed during initialization of the cluster.<br />   The script must be a GCS file with a gs:// prefix.</p></li><li><p><code>timeout_sec</code> - (Optional, Computed) The maximum duration (in seconds) which <code>script</code> is<br />   allowed to take to execute its action. GCP will default to a predetermined<br />   computed value if not set (currently 300).</p></li></ul><hr /><p><a name=\"nested_encryption_config\"></a>The <code>encryption_config</code> block supports:</p><p><code>hclcluster_config {  encryption_config {    kms_key_name = \"projects/projectId/locations/region/keyRings/keyRingName/cryptoKeys/keyName\"  }}</code></p><ul><li><code>kms_key_name</code> - (Required) The Cloud KMS key name to use for PD disk encryption for<br />   all instances in the cluster.</li></ul><hr /><p><a name=\"nested_dataproc_metric_config\"></a>The <code>dataproc_metric_config</code> block supports:</p><p><code>hcldataproc_metric_config {      metrics {        metric_source = \"HDFS\"        metric_overrides = [\"yarn:ResourceManager:QueueMetrics:AppsCompleted\"]      }    }</code></p><ul><li><p><code>metrics</code> - (Required) Metrics sources to enable.</p></li><li><p><code>metric_source</code> - (Required) A source for the collection of Dataproc OSS metrics (see <a href=\"https://cloud.google.com//dataproc/docs/guides/monitoring#available_oss_metrics\">available OSS metrics</a>).</p></li><li><p><code>metric_overrides</code> - (Optional) One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.</p></li></ul><hr /><p><a name=\"nested_lifecycle_config\"></a>The <code>lifecycle_config</code> block supports:</p><p><code>hclcluster_config {  lifecycle_config {    idle_delete_ttl = \"10m\"    auto_delete_time = \"2120-01-01T12:00:00.01Z\"  }}</code></p><ul><li><p><code>idle_delete_ttl</code> - (Optional) The duration to keep the cluster alive while idling<br />  (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].</p></li><li><p><code>auto_delete_time</code> - (Optional) The time when cluster will be auto-deleted.<br />  A timestamp in RFC3339 UTC \"Zulu\" format, accurate to nanoseconds.<br />  Example: \"2014-10-02T15:01:23.045123456Z\".</p></li></ul><hr /><p><a name=\"nested_endpoint_config\"></a>The <code>endpoint_config</code> block (Optional, Computed, Beta) supports:</p><p><code>hclcluster_config {  endpoint_config {    enable_http_port_access = \"true\"  }}</code></p><ul><li><code>enable_http_port_access</code> - (Optional) The flag to enable http access to specific ports<br />  on the cluster from external sources (aka Component Gateway). Defaults to false.</li></ul><p><a name=\"nested_metastore_config\"></a>The <code>metastore_config</code> block (Optional, Computed, Beta) supports:</p><p><code>hclcluster_config {  metastore_config {    dataproc_metastore_service = \"projects/projectId/locations/region/services/serviceName\"  }}</code></p><ul><li><code>dataproc_metastore_service</code> - (Required) Resource name of an existing Dataproc Metastore service.</li></ul><p>Only resource names including projectid and location (region) are valid. Examples:</p><p><code>projects/[projectId]/locations/[dataproc_region]/services/[service-name]</code></p>", "attributes-reference": "<h2 id=\"attributes-reference\">Attributes Reference</h2><p>In addition to the arguments listed above, the following computed attributes are<br />exported:</p><ul><li><p><code>cluster_config.0.master_config.0.instance_names</code> - List of master instance names which<br />   have been assigned to the cluster.</p></li><li><p><code>cluster_config.0.worker_config.0.instance_names</code> - List of worker instance names which have been assigned<br />    to the cluster.</p></li><li><p><code>cluster_config.0.preemptible_worker_config.0.instance_names</code> - List of preemptible instance names which have been assigned<br />    to the cluster.</p></li><li><p><code>cluster_config.0.bucket</code> - The name of the cloud storage bucket ultimately used to house the staging data<br />   for the cluster. If <code>staging_bucket</code> is specified, it will contain this value, otherwise<br />   it will be the auto generated name.</p></li><li><p><code>cluster_config.0.software_config.0.properties</code> - A list of the properties used to set the daemon config files.<br />   This will include any values supplied by the user via <code>cluster_config.software_config.override_properties</code></p></li><li><p><code>cluster_config.0.lifecycle_config.0.idle_start_time</code> - Time when the cluster became idle<br />  (most recent job finished) and became eligible for deletion due to idleness.</p></li><li><p><code>cluster_config.0.endpoint_config.0.http_ports</code> - The map of port descriptions to URLs. Will only be populated if<br /><code>enable_http_port_access</code> is true.</p></li></ul>", "import": "<h2 id=\"import\">Import</h2><p>This resource does not support import.</p>", "timeouts": "<h2 id=\"timeouts\">Timeouts</h2><p>This resource provides the following<br /><a href=\"https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts\">Timeouts</a> configuration options: configuration options:</p><ul><li><code>create</code> - Default is 45 minutes.</li><li><code>update</code> - Default is 45 minutes.</li><li><code>delete</code> - Default is 45 minutes.</li></ul>", "description": "<h1 id=\"google_dataproc_cluster\">google_dataproc_cluster</h1><p>Manages a Cloud Dataproc cluster resource within GCP.</p><ul><li><a href=\"https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.clusters\">API documentation</a></li><li>How-to Guides<ul><li><a href=\"https://cloud.google.com/dataproc/docs\">Official Documentation</a></li></ul></li></ul><p>!&gt; <strong>Warning:</strong> Due to limitations of the API, all arguments except<br /><code>labels</code>,<code>cluster_config.worker_config.num_instances</code> and <code>cluster_config.preemptible_worker_config.num_instances</code> are non-updatable. Changing others will cause recreation of the<br />whole cluster!</p>"}