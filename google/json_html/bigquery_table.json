{"google-bigquery-table": "<h1 id=\"google_bigquery_table\">google_bigquery_table</h1><p>Creates a table resource in a dataset for Google BigQuery. For more information see<br /><a href=\"https://cloud.google.com/bigquery/docs/\">the official documentation</a> and<br /><a href=\"https://cloud.google.com/bigquery/docs/reference/rest/v2/tables\">API</a>.</p><p>-&gt; <strong>Note</strong>: On newer versions of the provider, you must explicitly set <code>deletion_protection=false</code><br />(and run <code>terraform apply</code> to write the field to state) in order to destroy an instance.<br />It is recommended to not set this field (or set it to true) until you're ready to destroy.</p>", "example-usage": "<h2 id=\"example-usage\">Example Usage</h2><p>resource \"google_bigquery_dataset\" \"default\" {<br />  dataset_id                  = \"foo\"<br />  friendly_name               = \"test\"<br />  description                 = \"This is a test description\"<br />  location                    = \"EU\"<br />  default_table_expiration_ms = 3600000</p><br /><p>labels = {<br />    env = \"default\"<br />  }<br />}</p><br /><p>resource \"google_bigquery_table\" \"default\" {<br />  dataset_id = google_bigquery_dataset.default.dataset_id<br />  table_id   = \"bar\"</p><br /><p>time_partitioning {<br />    type = \"DAY\"<br />  }</p><br /><p>labels = {<br />    env = \"default\"<br />  }</p><br /><p>schema = &lt;&lt;EOF<br />[<br />  {<br />    \"name\": \"permalink\",<br />    \"type\": \"STRING\",<br />    \"mode\": \"NULLABLE\",<br />    \"description\": \"The Permalink\"<br />  },<br />  {<br />    \"name\": \"state\",<br />    \"type\": \"STRING\",<br />    \"mode\": \"NULLABLE\",<br />    \"description\": \"State where the head office is located\"<br />  }<br />]<br />EOF</p><br /><p>}</p><br /><p>resource \"google_bigquery_table\" \"sheet\" {<br />  dataset_id = google_bigquery_dataset.default.dataset_id<br />  table_id   = \"sheet\"</p><br /><p>external_data_configuration {<br />    autodetect    = true<br />    source_format = \"GOOGLE_SHEETS\"</p><br /><pre><code>google_sheets_options {<br />  skip_leading_rows = 1<br />}<br /><br />source_uris = [<br />  \"https://docs.google.com/spreadsheets/d/123456789012345\",<br />]<br /></code></pre><br /><p>}<br />}</p><br />", "argument-reference": "<h2 id=\"argument-reference\">Argument Reference</h2><p>The following arguments are supported:</p><ul><li><p><code>dataset_id</code> - (Required) The dataset ID to create the table in.<br />    Changing this forces a new resource to be created.</p></li><li><p><code>table_id</code> - (Required) A unique ID for the resource.<br />    Changing this forces a new resource to be created.</p></li><li><p><code>project</code> - (Optional) The ID of the project in which the resource belongs. If it<br />    is not provided, the provider project is used.</p></li><li><p><code>description</code> - (Optional) The field description.</p></li><li><p><code>expiration_time</code> - (Optional) The time when this table expires, in<br />    milliseconds since the epoch. If not present, the table will persist<br />    indefinitely. Expired tables will be deleted and their storage<br />    reclaimed.</p></li><li><p><code>external_data_configuration</code> - (Optional) Describes the data format,<br />    location, and other properties of a table stored outside of BigQuery.<br />    By defining these properties, the data source can then be queried as<br />    if it were a standard BigQuery table. Structure is <a href=\"#nested_external_data_configuration\">documented below</a>.</p></li><li><p><code>friendly_name</code> - (Optional) A descriptive name for the table.</p></li><li><p><code>max_staleness</code>: (Optional) The maximum staleness of data that could be returned when the table (or stale MV) is queried. Staleness encoded as a string encoding of sql IntervalValue type.</p></li><li><p><code>encryption_configuration</code> - (Optional) Specifies how the table should be encrypted.<br />    If left blank, the table will be encrypted with a Google-managed key; that process<br />    is transparent to the user.  Structure is <a href=\"#nested_encryption_configuration\">documented below</a>.</p></li><li><p><code>labels</code> - (Optional) A mapping of labels to assign to the resource.</p></li><li><p><a name=\"schema\"></a><code>schema</code> - (Optional) A JSON schema for the table.</p><p>~&gt;<strong>NOTE:</strong> Because this field expects a JSON string, any changes to the<br />string will create a diff, even if the JSON itself hasn't changed.<br />If the API returns a different value for the same schema, e.g. it<br />switched the order of values or replaced <code>STRUCT</code> field type with <code>RECORD</code><br />field type, we currently cannot suppress the recurring diff this causes.<br />As a workaround, we recommend using the schema as returned by the API.</p><p>~&gt;<strong>NOTE:</strong>  If you use <code>external_data_configuration</code><br /><a href=\"#nested_external_data_configuration\">documented below</a> and do <strong>not</strong> set<br /><code>external_data_configuration.connection_id</code>, schemas must be specified<br />with <code>external_data_configuration.schema</code>. Otherwise, schemas must be<br />specified with this top-level field.</p></li><li><p><code>time_partitioning</code> - (Optional) If specified, configures time-based<br />    partitioning for this table. Structure is <a href=\"#nested_time_partitioning\">documented below</a>.</p></li><li><p><code>range_partitioning</code> - (Optional) If specified, configures range-based<br />    partitioning for this table. Structure is <a href=\"#nested_range_partitioning\">documented below</a>.</p></li><li><p><code>clustering</code> - (Optional) Specifies column names to use for data clustering.<br />    Up to four top-level columns are allowed, and should be specified in<br />    descending priority order.</p></li><li><p><code>view</code> - (Optional) If specified, configures this table as a view.<br />    Structure is <a href=\"#nested_view\">documented below</a>.</p></li><li><p><code>materialized_view</code> - (Optional) If specified, configures this table as a materialized view.<br />    Structure is <a href=\"#nested_materialized_view\">documented below</a>.</p></li><li><p><code>deletion_protection</code> - (Optional) Whether or not to allow Terraform to destroy the instance. Unless this field is set to false<br />in Terraform state, a <code>terraform destroy</code> or <code>terraform apply</code> that would delete the instance will fail.</p></li><li><p><code>table_constraints</code> - (Optional) Defines the primary key and foreign keys. <br />    Structure is <a href=\"#nested_table_constraints\">documented below</a>.</p></li></ul><p><a name=\"nested_external_data_configuration\"></a>The <code>external_data_configuration</code> block supports:</p><ul><li><p><code>autodetect</code> - (Required) - Let BigQuery try to autodetect the schema<br />    and format of the table.</p></li><li><p><code>compression</code> (Optional) - The compression type of the data source.<br />    Valid values are \"NONE\" or \"GZIP\".</p></li><li><p><code>connection_id</code> (Optional) - The connection specifying the credentials to be used to read<br />    external storage, such as Azure Blob, Cloud Storage, or S3. The <code>connection_id</code> can have<br />    the form <code>{{project}}.{{location}}.{{connection_id}}</code><br />    or <code>projects/{{project}}/locations/{{location}}/connections/{{connection_id}}</code>.</p><p>~&gt;<strong>NOTE:</strong> If you set <code>external_data_configuration.connection_id</code>, the<br />table schema must be specified using the top-level <code>schema</code> field<br /><a href=\"#schema\">documented above</a>.</p></li><li><p><code>csv_options</code> (Optional) - Additional properties to set if<br /><code>source_format</code> is set to \"CSV\". Structure is <a href=\"#nested_csv_options\">documented below</a>.</p></li><li><p><code>json_options</code> (Optional) - Additional properties to set if<br /><code>source_format</code> is set to \"JSON\". Structure is <a href=\"#nested_json_options\">documented below</a>.</p></li><li><p><code>parquet_options</code> (Optional) - Additional properties to set if<br /><code>source_format</code> is set to \"PARQUET\". Structure is <a href=\"#nested_parquet_options\">documented below</a>.</p></li><li><p><code>google_sheets_options</code> (Optional) - Additional options if<br /><code>source_format</code> is set to \"GOOGLE_SHEETS\". Structure is<br /><a href=\"#nested_google_sheets_options\">documented below</a>.</p></li><li><p><code>hive_partitioning_options</code> (Optional) - When set, configures hive partitioning<br />    support. Not all storage formats support hive partitioning -- requesting hive<br />    partitioning on an unsupported format will lead to an error, as will providing<br />    an invalid specification. Structure is <a href=\"#nested_hive_partitioning_options\">documented below</a>.</p></li><li><p><code>avro_options</code> (Optional) - Additional options if <code>source_format</code> is set to<br />    \"AVRO\".  Structure is <a href=\"#nested_avro_options\">documented below</a>.</p></li><li><p><code>ignore_unknown_values</code> (Optional) - Indicates if BigQuery should<br />    allow extra values that are not represented in the table schema.<br />    If true, the extra values are ignored. If false, records with<br />    extra columns are treated as bad records, and if there are too<br />    many bad records, an invalid error is returned in the job result.<br />    The default value is false.</p></li><li><p><code>max_bad_records</code> (Optional) - The maximum number of bad records that<br />    BigQuery can ignore when reading data.</p></li><li><p><code>schema</code> - (Optional) A JSON schema for the external table. Schema is required<br />    for CSV and JSON formats if autodetect is not on. Schema is disallowed<br />    for Google Cloud Bigtable, Cloud Datastore backups, Avro, Iceberg, ORC and Parquet formats.<br />    ~&gt;<strong>NOTE:</strong> Because this field expects a JSON string, any changes to the<br />    string will create a diff, even if the JSON itself hasn't changed.<br />    Furthermore drift for this field cannot not be detected because BigQuery<br />    only uses this schema to compute the effective schema for the table, therefore<br />    any changes on the configured value will force the table to be recreated.<br />    This schema is effectively only applied when creating a table from an external<br />    datasource, after creation the computed schema will be stored in<br /><code>google_bigquery_table.schema</code></p><p>~&gt;<strong>NOTE:</strong> If you set <code>external_data_configuration.connection_id</code>, the<br />table schema must be specified using the top-level <code>schema</code> field<br /><a href=\"#schema\">documented above</a>.</p></li><li><p><code>source_format</code> (Optional) - The data format. Please see sourceFormat under<br /><a href=\"https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externaldataconfiguration\">ExternalDataConfiguration</a><br />    in Bigquery's public API documentation for supported formats. To use \"GOOGLE_SHEETS\"<br />    the <code>scopes</code> must include \"https://www.googleapis.com/auth/drive.readonly\".</p></li><li><p><code>source_uris</code> - (Required) A list of the fully-qualified URIs that point to<br />    your data in Google Cloud.</p></li><li><p><code>file_set_spec_type</code> - (Optional) Specifies how source URIs are interpreted for constructing the file set to load.<br />    By default source URIs are expanded against the underlying storage.<br />    Other options include specifying manifest files. Only applicable to object storage systems. <a href=\"cloud/bigquery/docs/reference/rest/v2/tables#filesetspectype\">Docs</a></p></li><li><p><code>reference_file_schema_uri</code> - (Optional) When creating an external table, the user can provide a reference file with the table schema. This is enabled for the following formats: AVRO, PARQUET, ORC.</p></li><li><p><code>metadata_cache_mode</code> - (Optional) Metadata Cache Mode for the table. Set this to enable caching of metadata from external data source. Valid values are <code>AUTOMATIC</code> and <code>MANUAL</code>.</p></li><li><p><code>object_metadata</code> - (Optional) Object Metadata is used to create Object Tables. Object Tables contain a listing of objects (with their metadata) found at the sourceUris. If <code>object_metadata</code> is set, <code>source_format</code> should be omitted.</p></li></ul><p><a name=\"nested_csv_options\"></a>The <code>csv_options</code> block supports:</p><ul><li><p><code>quote</code> (Required) - The value that is used to quote data sections in a<br />    CSV file. If your data does not contain quoted sections, set the<br />    property value to an empty string. If your data contains quoted newline<br />    characters, you must also set the <code>allow_quoted_newlines</code> property to true.<br />    The API-side default is <code>\"</code>, specified in Terraform escaped as <code>\\\"</code>. Due to<br />    limitations with Terraform default values, this value is required to be<br />    explicitly set.</p></li><li><p><code>allow_jagged_rows</code> (Optional) - Indicates if BigQuery should accept rows<br />    that are missing trailing optional columns.</p></li><li><p><code>allow_quoted_newlines</code> (Optional) - Indicates if BigQuery should allow<br />    quoted data sections that contain newline characters in a CSV file.<br />    The default value is false.</p></li><li><p><code>encoding</code> (Optional) - The character encoding of the data. The supported<br />    values are UTF-8 or ISO-8859-1.</p></li><li><p><code>field_delimiter</code> (Optional) - The separator for fields in a CSV file.</p></li><li><p><code>skip_leading_rows</code> (Optional) - The number of rows at the top of a CSV<br />    file that BigQuery will skip when reading the data.</p></li></ul><p><a name=\"nested_json_options\"></a>The <code>json_options</code> block supports:</p><ul><li><code>encoding</code> (Optional) - The character encoding of the data. The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default value is UTF-8.</li></ul><p><a name=\"nested_google_sheets_options\"></a>The <code>google_sheets_options</code> block supports:</p><ul><li><p><code>range</code> (Optional) - Range of a sheet to query from. Only used when<br />    non-empty. At least one of <code>range</code> or <code>skip_leading_rows</code> must be set.<br />    Typical format: \"sheet_name!top_left_cell_id:bottom_right_cell_id\"<br />    For example: \"sheet1!A1:B20\"</p></li><li><p><code>skip_leading_rows</code> (Optional) - The number of rows at the top of the sheet<br />    that BigQuery will skip when reading the data. At least one of <code>range</code> or<br /><code>skip_leading_rows</code> must be set.</p></li></ul><p><a name=\"nested_hive_partitioning_options\"></a>The <code>hive_partitioning_options</code> block supports:</p><ul><li><p><code>mode</code> (Optional) - When set, what mode of hive partitioning to use when<br />    reading data. The following modes are supported.</p><ul><li>AUTO: automatically infer partition key name(s) and type(s).</li><li>STRINGS: automatically infer partition key name(s). All types are<br />  Not all storage formats support hive partitioning. Requesting hive<br />  partitioning on an unsupported format will lead to an error.<br />  Currently supported formats are: JSON, CSV, ORC, Avro and Parquet.</li><li>CUSTOM: when set to <code>CUSTOM</code>, you must encode the partition key schema within the <code>source_uri_prefix</code> by setting <code>source_uri_prefix</code> to <code>gs://bucket/path_to_table/{key1:TYPE1}/{key2:TYPE2}/{key3:TYPE3}</code>.</li></ul></li><li><p><code>require_partition_filter</code> - (Optional) If set to true, queries over this table<br />    require a partition filter that can be used for partition elimination to be<br />    specified.</p></li><li><p><code>source_uri_prefix</code> (Optional) - When hive partition detection is requested,<br />    a common for all source uris must be required. The prefix must end immediately<br />    before the partition key encoding begins. For example, consider files following<br />    this data layout. <code>gs://bucket/path_to_table/dt=2019-06-01/country=USA/id=7/file.avro</code><br /><code>gs://bucket/path_to_table/dt=2019-05-31/country=CA/id=3/file.avro</code> When hive<br />    partitioning is requested with either AUTO or STRINGS detection, the common prefix<br />    can be either of <code>gs://bucket/path_to_table</code> or <code>gs://bucket/path_to_table/</code>.<br />    Note that when <code>mode</code> is set to <code>CUSTOM</code>, you must encode the partition key schema within the <code>source_uri_prefix</code> by setting <code>source_uri_prefix</code> to <code>gs://bucket/path_to_table/{key1:TYPE1}/{key2:TYPE2}/{key3:TYPE3}</code>.</p></li></ul><p><a name=\"nested_avro_options\"></a>The <code>avro_options</code> block supports:</p><ul><li><code>use_avro_logical_types</code> (Optional) - If is set to true, indicates whether<br />    to interpret logical types as the corresponding BigQuery data type<br />    (for example, TIMESTAMP), instead of using the raw type (for example, INTEGER).</li></ul><p><a name=\"nested_parquet_options\"></a>The <code>parquet_options</code> block supports:</p><ul><li><p><code>enum_as_string</code> (Optional) - Indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.</p></li><li><p><code>enable_list_inference</code> (Optional) - Indicates whether to use schema inference specifically for Parquet LIST logical type.</p></li></ul><p><a name=\"nested_time_partitioning\"></a>The <code>time_partitioning</code> block supports:</p><ul><li><p><code>expiration_ms</code> -  (Optional) Number of milliseconds for which to keep the<br />    storage for a partition.</p></li><li><p><code>field</code> - (Optional) The field used to determine how to create a time-based<br />    partition. If time-based partitioning is enabled without this value, the<br />    table is partitioned based on the load time.</p></li><li><p><code>type</code> - (Required) The supported types are DAY, HOUR, MONTH, and YEAR,<br />    which will generate one partition per day, hour, month, and year, respectively.</p></li><li><p><code>require_partition_filter</code> - (Optional) If set to true, queries over this table<br />    require a partition filter that can be used for partition elimination to be<br />    specified.</p></li></ul><p><a name=\"nested_range_partitioning\"></a>The <code>range_partitioning</code> block supports:</p><ul><li><p><code>field</code> - (Required) The field used to determine how to create a range-based<br />    partition.</p></li><li><p><code>range</code> - (Required) Information required to partition based on ranges.<br />    Structure is <a href=\"#nested_range\">documented below</a>.</p></li></ul><p><a name=\"nested_range\"></a>The <code>range</code> block supports:</p><ul><li><p><code>start</code> - (Required) Start of the range partitioning, inclusive.</p></li><li><p><code>end</code> - (Required) End of the range partitioning, exclusive.</p></li><li><p><code>interval</code> - (Required) The width of each range within the partition.</p></li></ul><p><a name=\"nested_view\"></a>The <code>view</code> block supports:</p><ul><li><p><code>query</code> - (Required) A query that BigQuery executes when the view is referenced.</p></li><li><p><code>use_legacy_sql</code> - (Optional) Specifies whether to use BigQuery's legacy SQL for this view.<br />    The default value is true. If set to false, the view will use BigQuery's standard SQL.</p></li></ul><p><a name=\"nested_materialized_view\"></a>The <code>materialized_view</code> block supports:</p><ul><li><p><code>query</code> - (Required) A query whose result is persisted.</p></li><li><p><code>enable_refresh</code> - (Optional) Specifies whether to use BigQuery's automatic refresh for this materialized view when the base table is updated.<br />    The default value is true.</p></li><li><p><code>refresh_interval_ms</code> - (Optional) The maximum frequency at which this materialized view will be refreshed.<br />    The default value is 1800000</p></li><li><p><code>allow_non_incremental_definition</code> - (Optional) Allow non incremental materialized view definition.<br />    The default value is false.</p></li></ul><p><a name=\"nested_encryption_configuration\"></a>The <code>encryption_configuration</code> block supports the following arguments:</p><ul><li><code>kms_key_name</code> - (Required) The self link or full name of a key which should be used to<br />    encrypt this table.  Note that the default bigquery service account will need to have<br />    encrypt/decrypt permissions on this key - you may want to see the<br /><code>google_bigquery_default_service_account</code> datasource and the<br /><code>google_kms_crypto_key_iam_binding</code> resource.</li></ul><p><a name=\"nested_table_constraints\"></a>The <code>table_constraints</code> block supports:</p><ul><li><p><code>primary_key</code> - (Optional) Represents the primary key constraint<br />    on a table's columns. Present only if the table has a primary key.<br />    The primary key is not enforced.<br />    Structure is <a href=\"#nested_primary_key\">documented below</a>.</p></li><li><p><code>foreign_keys</code> - (Optional) Present only if the table has a foreign key.<br />    The foreign key is not enforced.<br />    Structure is <a href=\"#nested_foreign_keys\">documented below</a>.</p></li></ul><p><a name=\"nested_primary_key\"></a>The <code>primary_key</code> block supports:</p><ul><li><code>columns</code>: (Required) The columns that are composed of the primary key constraint.</li></ul><p><a name=\"nested_foreign_keys\"></a>The <code>foreign_keys</code> block supports:</p><ul><li><p><code>name</code>: (Optional) Set only if the foreign key constraint is named.</p></li><li><p><code>referenced_table</code>: (Required) The table that holds the primary key<br />    and is referenced by this foreign key.<br />    Structure is <a href=\"#nested_referenced_table\">documented below</a>.</p></li><li><p><code>column_references</code>: (Required) The pair of the foreign key column and primary key column.<br />    Structure is <a href=\"#nested_column_references\">documented below</a>.</p></li></ul><p><a name=\"nested_referenced_table\"></a>The <code>referenced_table</code> block supports:</p><ul><li><p><code>project_id</code>: (Required) The ID of the project containing this table.</p></li><li><p><code>dataset_id</code>: (Required) The ID of the dataset containing this table.</p></li><li><p><code>table_id</code>: (Required) The ID of the table. The ID must contain only<br />    letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum<br />    length is 1,024 characters. Certain operations allow suffixing of<br />    the table ID with a partition decorator, such as<br />    sample_table$20190123.</p></li></ul><p><a name=\"nested_column_references\"></a>The <code>column_references</code> block supports:</p><ul><li><p><code>referencing_column</code>: (Required) The column that composes the foreign key.</p></li><li><p><code>referenced_column</code>: (Required) The column in the primary key that are<br />    referenced by the referencingColumn</p></li></ul>", "attributes-reference": "<h2 id=\"attributes-reference\">Attributes Reference</h2><p>In addition to the arguments listed above, the following computed attributes are<br />exported:</p><ul><li><p><code>id</code> - an identifier for the resource with format <code>projects/{{project}}/datasets/{{dataset}}/tables/{{name}}</code></p></li><li><p><code>creation_time</code> - The time when this table was created, in milliseconds since the epoch.</p></li><li><p><code>etag</code> - A hash of the resource.</p></li><li><p><code>kms_key_version</code> - The self link or full name of the kms key version used to encrypt this table.</p></li><li><p><code>last_modified_time</code> - The time when this table was last modified, in milliseconds since the epoch.</p></li><li><p><code>location</code> - The geographic location where the table resides. This value is inherited from the dataset.</p></li><li><p><code>num_bytes</code> - The size of this table in bytes, excluding any data in the streaming buffer.</p></li><li><p><code>num_long_term_bytes</code> - The number of bytes in the table that are considered \"long-term storage\".</p></li><li><p><code>num_rows</code> - The number of rows of data in this table, excluding any data in the streaming buffer.</p></li><li><p><code>self_link</code> - The URI of the created resource.</p></li><li><p><code>type</code> - Describes the table type.</p></li></ul>", "import": "<h2 id=\"import\">Import</h2><p>BigQuery tables imported using any of these accepted formats:</p><p>$ terraform import google_bigquery_table.default projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}<br />$ terraform import google_bigquery_table.default {{project}}/{{dataset_id}}/{{table_id}}<br />$ terraform import google_bigquery_table.default {{dataset_id}}/{{table_id}}</p>", "description": "<h1 id=\"google_bigquery_table\">google_bigquery_table</h1><p>Creates a table resource in a dataset for Google BigQuery. For more information see<br /><a href=\"https://cloud.google.com/bigquery/docs/\">the official documentation</a> and<br /><a href=\"https://cloud.google.com/bigquery/docs/reference/rest/v2/tables\">API</a>.</p><p>-&gt; <strong>Note</strong>: On newer versions of the provider, you must explicitly set <code>deletion_protection=false</code><br />(and run <code>terraform apply</code> to write the field to state) in order to destroy an instance.<br />It is recommended to not set this field (or set it to true) until you're ready to destroy.</p>"}