{"": "<h1 id=\"-_2\">----------------------------------------------------------------------------</h1><p>subcategory: \"BigQuery\"<br />description: |-<br />  Jobs are actions that BigQuery runs on your behalf to load data, export data, query data, or copy data.</p><hr />", "auto-generated-code-type-mmv1": "<h1 id=\"auto-generated-code-type-mmv1\"><strong><em>     AUTO GENERATED CODE    </em></strong>    Type: MMv1     ***</h1>", "this-file-is-automatically-generated-by-magic-modules-and-manual": "<h1 id=\"this-file-is-automatically-generated-by-magic-modules-and-manual\">This file is automatically generated by Magic Modules and manual</h1>", "changes-will-be-clobbered-when-the-file-is-regenerated": "<h1 id=\"changes-will-be-clobbered-when-the-file-is-regenerated\">changes will be clobbered when the file is regenerated.</h1>", "please-read-more-about-how-to-change-this-file-in": "<h1 id=\"please-read-more-about-how-to-change-this-file-in\">Please read more about how to change this file in</h1>", "github-contributing-md": "<h1 id=\"githubcontributingmd\">.github/CONTRIBUTING.md.</h1>", "google-bigquery-job": "<h1 id=\"google_bigquery_job\">google_bigquery_job</h1><p>Jobs are actions that BigQuery runs on your behalf to load data, export data, query data, or copy data.<br />Once a BigQuery job is created, it cannot be changed or deleted.</p><p>To get more information about Job, see:</p><ul><li><a href=\"https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs\">API documentation</a></li><li>How-to Guides<ul><li><a href=\"https://cloud.google.com/bigquery/docs/jobs-overview\">BigQuery Jobs Intro</a></li></ul></li></ul><div class = \"oics-button\" style=\"float: right; margin: 0 0 -15px\">  <a href=\"https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=bigquery_job_query&cloudshell_image=gcr.io%2Fcloudshell-images%2Fcloudshell%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md\" target=\"_blank\">    <img alt=\"Open in Cloud Shell\" src=\"//gstatic.com/cloudssh/images/open-btn.svg\" style=\"max-height: 44px; margin: 32px auto; max-width: 100%;\">  </a></div>", "example-usage---bigquery-job-query": "<h2 id=\"example-usage-bigquery-job-query\">Example Usage - Bigquery Job Query</h2><p><br />resource \"google_bigquery_table\" \"foo\" {<br />  deletion_protection = false<br />  dataset_id = google_bigquery_dataset.bar.dataset_id<br />  table_id   = \"job_query_table\"<br />}</p><p>resource \"google_bigquery_dataset\" \"bar\" {<br />  dataset_id                  = \"job_query_dataset\"<br />  friendly_name               = \"test\"<br />  description                 = \"This is a test description\"<br />  location                    = \"US\"<br />}</p><p>resource \"google_bigquery_job\" \"job\" {<br />  job_id     = \"job_query\"</p><p>labels = {<br />    \"example-label\" =\"example-value\"<br />  }</p><p>query {<br />    query = \"SELECT state FROM [lookerdata:cdc.project_tycho_reports]\"</p><pre><code>destination_table {  project_id = google_bigquery_table.foo.project  dataset_id = google_bigquery_table.foo.dataset_id  table_id   = google_bigquery_table.foo.table_id}allow_large_results = trueflatten_results = truescript_options {  key_result_statement = \"LAST\"}</code></pre><p>}<br />}<br /></p><div class = \"oics-button\" style=\"float: right; margin: 0 0 -15px\">  <a href=\"https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=bigquery_job_query_table_reference&cloudshell_image=gcr.io%2Fcloudshell-images%2Fcloudshell%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md\" target=\"_blank\">    <img alt=\"Open in Cloud Shell\" src=\"//gstatic.com/cloudssh/images/open-btn.svg\" style=\"max-height: 44px; margin: 32px auto; max-width: 100%;\">  </a></div>", "example-usage---bigquery-job-query-table-reference": "<h2 id=\"example-usage-bigquery-job-query-table-reference\">Example Usage - Bigquery Job Query Table Reference</h2><p><br />resource \"google_bigquery_table\" \"foo\" {<br />  deletion_protection = false<br />  dataset_id = google_bigquery_dataset.bar.dataset_id<br />  table_id   = \"job_query_table\"<br />}</p><p>resource \"google_bigquery_dataset\" \"bar\" {<br />  dataset_id                  = \"job_query_dataset\"<br />  friendly_name               = \"test\"<br />  description                 = \"This is a test description\"<br />  location                    = \"US\"<br />}</p><p>resource \"google_bigquery_job\" \"job\" {<br />  job_id     = \"job_query\"</p><p>labels = {<br />    \"example-label\" =\"example-value\"<br />  }</p><p>query {<br />    query = \"SELECT state FROM [lookerdata:cdc.project_tycho_reports]\"</p><pre><code>destination_table {  table_id = google_bigquery_table.foo.id}default_dataset {  dataset_id = google_bigquery_dataset.bar.id}allow_large_results = trueflatten_results = truescript_options {  key_result_statement = \"LAST\"}</code></pre><p>}<br />}<br /></p><div class = \"oics-button\" style=\"float: right; margin: 0 0 -15px\">  <a href=\"https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=bigquery_job_load&cloudshell_image=gcr.io%2Fcloudshell-images%2Fcloudshell%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md\" target=\"_blank\">    <img alt=\"Open in Cloud Shell\" src=\"//gstatic.com/cloudssh/images/open-btn.svg\" style=\"max-height: 44px; margin: 32px auto; max-width: 100%;\">  </a></div>", "example-usage---bigquery-job-load": "<h2 id=\"example-usage-bigquery-job-load\">Example Usage - Bigquery Job Load</h2><p><br />resource \"google_bigquery_table\" \"foo\" {<br />  deletion_protection = false<br />  dataset_id = google_bigquery_dataset.bar.dataset_id<br />  table_id   = \"job_load_table\"<br />}</p><p>resource \"google_bigquery_dataset\" \"bar\" {<br />  dataset_id                  = \"job_load_dataset\"<br />  friendly_name               = \"test\"<br />  description                 = \"This is a test description\"<br />  location                    = \"US\"<br />}</p><p>resource \"google_bigquery_job\" \"job\" {<br />  job_id     = \"job_load\"</p><p>labels = {<br />    \"my_job\" =\"load\"<br />  }</p><p>load {<br />    source_uris = [<br />      \"gs://cloud-samples-data/bigquery/us-states/us-states-by-date.csv\",<br />    ]</p><pre><code>destination_table {  project_id = google_bigquery_table.foo.project  dataset_id = google_bigquery_table.foo.dataset_id  table_id   = google_bigquery_table.foo.table_id}skip_leading_rows = 1schema_update_options = [\"ALLOW_FIELD_RELAXATION\", \"ALLOW_FIELD_ADDITION\"]write_disposition = \"WRITE_APPEND\"autodetect = true</code></pre><p>}<br />}<br /></p>", "example-usage---bigquery-job-load-geojson": "<h2 id=\"example-usage-bigquery-job-load-geojson\">Example Usage - Bigquery Job Load Geojson</h2><p><br />locals {<br />  project = \"my-project-name\" # Google Cloud Platform Project ID<br />}</p><p>resource \"google_storage_bucket\" \"bucket\" {<br />  name     = \"${local.project}-bq-geojson\"  # Every bucket name must be globally unique<br />  location = \"US\"<br />  uniform_bucket_level_access = true<br />}</p><p>resource \"google_storage_bucket_object\" \"object\" {<br />  name   = \"geojson-data.jsonl\"<br />  bucket = google_storage_bucket.bucket.name<br />  content = &lt;&lt;EOF<br />{\"type\":\"Feature\",\"properties\":{\"continent\":\"Europe\",\"region\":\"Scandinavia\"},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-30.94,53.33],[33.05,53.33],[33.05,71.86],[-30.94,71.86],[-30.94,53.33]]]}}<br />{\"type\":\"Feature\",\"properties\":{\"continent\":\"Africa\",\"region\":\"West Africa\"},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-23.91,0],[11.95,0],[11.95,18.98],[-23.91,18.98],[-23.91,0]]]}}<br />EOF<br />}</p><p>resource \"google_bigquery_table\" \"foo\" {<br />  deletion_protection = false<br />  dataset_id = google_bigquery_dataset.bar.dataset_id<br />  table_id   = \"job_load_table\"<br />}</p><p>resource \"google_bigquery_dataset\" \"bar\" {<br />  dataset_id                  = \"job_load_dataset\"<br />  friendly_name               = \"test\"<br />  description                 = \"This is a test description\"<br />  location                    = \"US\"<br />}</p><p>resource \"google_bigquery_job\" \"job\" {<br />  job_id     = \"job_load\"</p><p>labels = {<br />    \"my_job\" = \"load\"<br />  }</p><p>load {<br />    source_uris = [<br />      \"gs://${google_storage_bucket_object.object.bucket}/${google_storage_bucket_object.object.name}\"<br />    ]</p><pre><code>destination_table {  project_id = google_bigquery_table.foo.project  dataset_id = google_bigquery_table.foo.dataset_id  table_id   = google_bigquery_table.foo.table_id}write_disposition = \"WRITE_TRUNCATE\"autodetect = truesource_format = \"NEWLINE_DELIMITED_JSON\"json_extension = \"GEOJSON\"</code></pre><p>}</p><p>depends_on = [\"google_storage_bucket_object.object\"]<br />}<br /></p><div class = \"oics-button\" style=\"float: right; margin: 0 0 -15px\">  <a href=\"https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=bigquery_job_load_parquet&cloudshell_image=gcr.io%2Fcloudshell-images%2Fcloudshell%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md\" target=\"_blank\">    <img alt=\"Open in Cloud Shell\" src=\"//gstatic.com/cloudssh/images/open-btn.svg\" style=\"max-height: 44px; margin: 32px auto; max-width: 100%;\">  </a></div>", "example-usage---bigquery-job-load-parquet": "<h2 id=\"example-usage-bigquery-job-load-parquet\">Example Usage - Bigquery Job Load Parquet</h2><p><br />resource \"google_storage_bucket\" \"test\" {<br />  name                        = \"job_load_bucket\"<br />  location                    = \"US\"<br />  uniform_bucket_level_access = true<br />}</p><p>resource \"google_storage_bucket_object\" \"test\" {<br />  name   =  \"job_load_bucket_object\"<br />  source = \"./test-fixtures/test.parquet.gzip\"<br />  bucket = google_storage_bucket.test.name<br />}</p><p>resource \"google_bigquery_dataset\" \"test\" {<br />  dataset_id                  = \"job_load_dataset\"<br />  friendly_name               = \"test\"<br />  description                 = \"This is a test description\"<br />  location                    = \"US\"<br />}</p><p>resource \"google_bigquery_table\" \"test\" {<br />  deletion_protection = false<br />  table_id            = \"job_load_table\"<br />  dataset_id          = google_bigquery_dataset.test.dataset_id<br />}</p><p>resource \"google_bigquery_job\" \"job\" {<br />  job_id = \"job_load\"</p><p>labels = {<br />    \"my_job\" =\"load\"<br />  }</p><p>load {<br />    source_uris = [<br />      \"gs://${google_storage_bucket_object.test.bucket}/${google_storage_bucket_object.test.name}\"<br />    ]</p><pre><code>destination_table {  project_id = google_bigquery_table.test.project  dataset_id = google_bigquery_table.test.dataset_id  table_id   = google_bigquery_table.test.table_id}schema_update_options = [\"ALLOW_FIELD_RELAXATION\", \"ALLOW_FIELD_ADDITION\"]write_disposition     = \"WRITE_APPEND\"source_format         = \"PARQUET\"autodetect            = trueparquet_options {  enum_as_string        = true  enable_list_inference = true}</code></pre><p>}<br />}<br /></p>", "example-usage---bigquery-job-copy": "<h2 id=\"example-usage-bigquery-job-copy\">Example Usage - Bigquery Job Copy</h2><p><br />resource \"google_bigquery_table\" \"source\" {<br />  deletion_protection = false<br />  count = length(google_bigquery_dataset.source)</p><p>dataset_id = google_bigquery_dataset.source[count.index].dataset_id<br />  table_id   = \"job_copy_${count.index}_table\"</p><p>schema = &lt;&lt;EOF<br />[<br />  {<br />    \"name\": \"name\",<br />    \"type\": \"STRING\",<br />    \"mode\": \"NULLABLE\"<br />  },<br />  {<br />    \"name\": \"post_abbr\",<br />    \"type\": \"STRING\",<br />    \"mode\": \"NULLABLE\"<br />  },<br />  {<br />    \"name\": \"date\",<br />    \"type\": \"DATE\",<br />    \"mode\": \"NULLABLE\"<br />  }<br />]<br />EOF<br />}</p><p>resource \"google_bigquery_dataset\" \"source\" {<br />  count = 2</p><p>dataset_id                  = \"job_copy_${count.index}_dataset\"<br />  friendly_name               = \"test\"<br />  description                 = \"This is a test description\"<br />  location                    = \"US\"<br />}</p><p>resource \"google_bigquery_table\" \"dest\" {<br />  deletion_protection = false<br />  dataset_id = google_bigquery_dataset.dest.dataset_id<br />  table_id   = \"job_copy_dest_table\"</p><p>schema = &lt;&lt;EOF<br />[<br />  {<br />    \"name\": \"name\",<br />    \"type\": \"STRING\",<br />    \"mode\": \"NULLABLE\"<br />  },<br />  {<br />    \"name\": \"post_abbr\",<br />    \"type\": \"STRING\",<br />    \"mode\": \"NULLABLE\"<br />  },<br />  {<br />    \"name\": \"date\",<br />    \"type\": \"DATE\",<br />    \"mode\": \"NULLABLE\"<br />  }<br />]<br />EOF</p><p>encryption_configuration {<br />    kms_key_name = google_kms_crypto_key.crypto_key.id<br />  }</p><p>depends_on = [\"google_project_iam_member.encrypt_role\"]<br />}</p><p>resource \"google_bigquery_dataset\" \"dest\" {<br />  dataset_id    = \"job_copy_dest_dataset\"<br />  friendly_name = \"test\"<br />  description   = \"This is a test description\"<br />  location      = \"US\"<br />}</p><p>resource \"google_kms_crypto_key\" \"crypto_key\" {<br />  name     = \"example-key\"<br />  key_ring = google_kms_key_ring.key_ring.id<br />}</p><p>resource \"google_kms_key_ring\" \"key_ring\" {<br />  name     = \"example-keyring\"<br />  location = \"global\"<br />}</p><p>data \"google_project\" \"project\" {<br />  project_id = \"my-project-name\"<br />}</p><p>resource \"google_project_iam_member\" \"encrypt_role\" {<br />  project = data.google_project.project.project_id<br />  role = \"roles/cloudkms.cryptoKeyEncrypterDecrypter\"<br />  member = \"serviceAccount:bq-${data.google_project.project.number}@bigquery-encryption.iam.gserviceaccount.com\"<br />}</p><p>resource \"google_bigquery_job\" \"job\" {<br />  job_id     = \"job_copy\"</p><p>copy {<br />    source_tables {<br />      project_id = google_bigquery_table.source.0.project<br />      dataset_id = google_bigquery_table.source.0.dataset_id<br />      table_id   = google_bigquery_table.source.0.table_id<br />    }</p><pre><code>source_tables {  project_id = google_bigquery_table.source.1.project  dataset_id = google_bigquery_table.source.1.dataset_id  table_id   = google_bigquery_table.source.1.table_id}destination_table {  project_id = google_bigquery_table.dest.project  dataset_id = google_bigquery_table.dest.dataset_id  table_id   = google_bigquery_table.dest.table_id}destination_encryption_configuration {  kms_key_name = google_kms_crypto_key.crypto_key.id}</code></pre><p>}</p><p>depends_on = [\"google_project_iam_member.encrypt_role\"]<br />}<br /></p><div class = \"oics-button\" style=\"float: right; margin: 0 0 -15px\">  <a href=\"https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=bigquery_job_extract&cloudshell_image=gcr.io%2Fcloudshell-images%2Fcloudshell%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md\" target=\"_blank\">    <img alt=\"Open in Cloud Shell\" src=\"//gstatic.com/cloudssh/images/open-btn.svg\" style=\"max-height: 44px; margin: 32px auto; max-width: 100%;\">  </a></div>", "example-usage---bigquery-job-extract": "<h2 id=\"example-usage-bigquery-job-extract\">Example Usage - Bigquery Job Extract</h2><p><br />resource \"google_bigquery_table\" \"source-one\" {<br />  deletion_protection = false<br />  dataset_id = google_bigquery_dataset.source-one.dataset_id<br />  table_id   = \"job_extract_table\"</p><p>schema = &lt;&lt;EOF<br />[<br />  {<br />    \"name\": \"name\",<br />    \"type\": \"STRING\",<br />    \"mode\": \"NULLABLE\"<br />  },<br />  {<br />    \"name\": \"post_abbr\",<br />    \"type\": \"STRING\",<br />    \"mode\": \"NULLABLE\"<br />  },<br />  {<br />    \"name\": \"date\",<br />    \"type\": \"DATE\",<br />    \"mode\": \"NULLABLE\"<br />  }<br />]<br />EOF<br />}</p><p>resource \"google_bigquery_dataset\" \"source-one\" {<br />  dataset_id    = \"job_extract_dataset\"<br />  friendly_name = \"test\"<br />  description   = \"This is a test description\"<br />  location      = \"US\"<br />}</p><p>resource \"google_storage_bucket\" \"dest\" {<br />  name          = \"job_extract_bucket\"<br />  location      = \"US\"<br />  force_destroy = true<br />}</p><p>resource \"google_bigquery_job\" \"job\" {<br />  job_id     = \"job_extract\"</p><p>extract {<br />    destination_uris = [\"${google_storage_bucket.dest.url}/extract\"]</p><pre><code>source_table {  project_id = google_bigquery_table.source-one.project  dataset_id = google_bigquery_table.source-one.dataset_id  table_id   = google_bigquery_table.source-one.table_id}destination_format = \"NEWLINE_DELIMITED_JSON\"compression = \"GZIP\"</code></pre><p>}<br />}<br /></p>", "argument-reference": "<h2 id=\"argument-reference\">Argument Reference</h2><p>The following arguments are supported:</p><ul><li><code>job_id</code> -<br />  (Required)<br />  The ID of the job. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024 characters.</li></ul><p><a name=\"nested_query\"></a>The <code>query</code> block supports:</p><ul><li><p><code>query</code> -<br />  (Required)<br />  SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.<br /><em>NOTE</em>: queries containing <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language\">DML language</a><br />  (<code>DELETE</code>, <code>UPDATE</code>, <code>MERGE</code>, <code>INSERT</code>) must specify <code>create_disposition = \"\"</code> and <code>write_disposition = \"\"</code>.</p></li><li><p><code>destination_table</code> -<br />  (Optional)<br />  Describes the table where the query results should be stored.<br />  This property must be set for large results that exceed the maximum response size.<br />  For queries that produce anonymous (cached) results, this field will be populated by BigQuery.<br />  Structure is <a href=\"#nested_destination_table\">documented below</a>.</p></li><li><p><code>user_defined_function_resources</code> -<br />  (Optional)<br />  Describes user-defined function resources used in the query.<br />  Structure is <a href=\"#nested_user_defined_function_resources\">documented below</a>.</p></li><li><p><code>create_disposition</code> -<br />  (Optional)<br />  Specifies whether the job is allowed to create new tables. The following values are supported:<br />  CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.<br />  CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.<br />  Creation, truncation and append actions occur as one atomic update upon job completion<br />  Default value is <code>CREATE_IF_NEEDED</code>.<br />  Possible values are: <code>CREATE_IF_NEEDED</code>, <code>CREATE_NEVER</code>.</p></li><li><p><code>write_disposition</code> -<br />  (Optional)<br />  Specifies the action that occurs if the destination table already exists. The following values are supported:<br />  WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.<br />  WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.<br />  WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.<br />  Each action is atomic and only occurs if BigQuery is able to complete the job successfully.<br />  Creation, truncation and append actions occur as one atomic update upon job completion.<br />  Default value is <code>WRITE_EMPTY</code>.<br />  Possible values are: <code>WRITE_TRUNCATE</code>, <code>WRITE_APPEND</code>, <code>WRITE_EMPTY</code>.</p></li><li><p><code>default_dataset</code> -<br />  (Optional)<br />  Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.<br />  Structure is <a href=\"#nested_default_dataset\">documented below</a>.</p></li><li><p><code>priority</code> -<br />  (Optional)<br />  Specifies a priority for the query.<br />  Default value is <code>INTERACTIVE</code>.<br />  Possible values are: <code>INTERACTIVE</code>, <code>BATCH</code>.</p></li><li><p><code>allow_large_results</code> -<br />  (Optional)<br />  If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.<br />  Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.<br />  However, you must still set destinationTable when result size exceeds the allowed maximum response size.</p></li><li><p><code>use_query_cache</code> -<br />  (Optional)<br />  Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever<br />  tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.<br />  The default value is true.</p></li><li><p><code>flatten_results</code> -<br />  (Optional)<br />  If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.<br />  allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.</p></li><li><p><code>maximum_billing_tier</code> -<br />  (Optional)<br />  Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).<br />  If unspecified, this will be set to your project default.</p></li><li><p><code>maximum_bytes_billed</code> -<br />  (Optional)<br />  Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).<br />  If unspecified, this will be set to your project default.</p></li><li><p><code>use_legacy_sql</code> -<br />  (Optional)<br />  Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.<br />  If set to false, the query will use BigQuery's standard SQL.</p></li><li><p><code>parameter_mode</code> -<br />  (Optional)<br />  Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.</p></li><li><p><code>schema_update_options</code> -<br />  (Optional)<br />  Allows the schema of the destination table to be updated as a side effect of the query job.<br />  Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;<br />  when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,<br />  specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.<br />  One or more of the following values are specified:<br />  ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.<br />  ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.</p></li><li><p><code>destination_encryption_configuration</code> -<br />  (Optional)<br />  Custom encryption configuration (e.g., Cloud KMS keys)<br />  Structure is <a href=\"#nested_destination_encryption_configuration\">documented below</a>.</p></li><li><p><code>script_options</code> -<br />  (Optional)<br />  Options controlling the execution of scripts.<br />  Structure is <a href=\"#nested_script_options\">documented below</a>.</p></li></ul><p><a name=\"nested_destination_table\"></a>The <code>destination_table</code> block supports:</p><ul><li><p><code>project_id</code> -<br />  (Optional)<br />  The ID of the project containing this table.</p></li><li><p><code>dataset_id</code> -<br />  (Optional)<br />  The ID of the dataset containing this table.</p></li><li><p><code>table_id</code> -<br />  (Required)<br />  The table. Can be specified <code>{{table_id}}</code> if <code>project_id</code> and <code>dataset_id</code> are also set,<br />  or of the form <code>projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}</code> if not.</p></li></ul><p><a name=\"nested_user_defined_function_resources\"></a>The <code>user_defined_function_resources</code> block supports:</p><ul><li><p><code>resource_uri</code> -<br />  (Optional)<br />  A code resource to load from a Google Cloud Storage URI (gs://bucket/path).</p></li><li><p><code>inline_code</code> -<br />  (Optional)<br />  An inline resource that contains code for a user-defined function (UDF).<br />  Providing a inline code resource is equivalent to providing a URI for a file containing the same code.</p></li></ul><p><a name=\"nested_default_dataset\"></a>The <code>default_dataset</code> block supports:</p><ul><li><p><code>dataset_id</code> -<br />  (Required)<br />  The dataset. Can be specified <code>{{dataset_id}}</code> if <code>project_id</code> is also set,<br />  or of the form <code>projects/{{project}}/datasets/{{dataset_id}}</code> if not.</p></li><li><p><code>project_id</code> -<br />  (Optional)<br />  The ID of the project containing this table.</p></li></ul><p><a name=\"nested_destination_encryption_configuration\"></a>The <code>destination_encryption_configuration</code> block supports:</p><ul><li><p><code>kms_key_name</code> -<br />  (Required)<br />  Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.<br />  The BigQuery Service Account associated with your project requires access to this encryption key.</p></li><li><p><code>kms_key_version</code> -<br />  (Output)<br />  Describes the Cloud KMS encryption key version used to protect destination BigQuery table.</p></li></ul><p><a name=\"nested_script_options\"></a>The <code>script_options</code> block supports:</p><ul><li><p><code>statement_timeout_ms</code> -<br />  (Optional)<br />  Timeout period for each statement in a script.</p></li><li><p><code>statement_byte_budget</code> -<br />  (Optional)<br />  Limit on the number of bytes billed per statement. Exceeding this budget results in an error.</p></li><li><p><code>key_result_statement</code> -<br />  (Optional)<br />  Determines which statement in the script represents the \"key result\",<br />  used to populate the schema and query results of the script job.<br />  Possible values are: <code>LAST</code>, <code>FIRST_SELECT</code>.</p></li></ul><p><a name=\"nested_load\"></a>The <code>load</code> block supports:</p><ul><li><p><code>source_uris</code> -<br />  (Required)<br />  The fully-qualified URIs that point to your data in Google Cloud.<br />  For Google Cloud Storage URIs: Each URI can contain one '*' wildcard character<br />  and it must come after the 'bucket' name. Size limits related to load jobs apply<br />  to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be<br />  specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.<br />  For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '*' wildcard character is not allowed.</p></li><li><p><code>destination_table</code> -<br />  (Required)<br />  The destination table to load the data into.<br />  Structure is <a href=\"#nested_destination_table\">documented below</a>.</p></li><li><p><code>create_disposition</code> -<br />  (Optional)<br />  Specifies whether the job is allowed to create new tables. The following values are supported:<br />  CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.<br />  CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.<br />  Creation, truncation and append actions occur as one atomic update upon job completion<br />  Default value is <code>CREATE_IF_NEEDED</code>.<br />  Possible values are: <code>CREATE_IF_NEEDED</code>, <code>CREATE_NEVER</code>.</p></li><li><p><code>write_disposition</code> -<br />  (Optional)<br />  Specifies the action that occurs if the destination table already exists. The following values are supported:<br />  WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.<br />  WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.<br />  WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.<br />  Each action is atomic and only occurs if BigQuery is able to complete the job successfully.<br />  Creation, truncation and append actions occur as one atomic update upon job completion.<br />  Default value is <code>WRITE_EMPTY</code>.<br />  Possible values are: <code>WRITE_TRUNCATE</code>, <code>WRITE_APPEND</code>, <code>WRITE_EMPTY</code>.</p></li><li><p><code>null_marker</code> -<br />  (Optional)<br />  Specifies a string that represents a null value in a CSV file. For example, if you specify \"\\N\", BigQuery interprets \"\\N\" as a null value<br />  when loading a CSV file. The default value is the empty string. If you set this property to a custom value, BigQuery throws an error if an<br />  empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as<br />  an empty value.</p></li><li><p><code>field_delimiter</code> -<br />  (Optional)<br />  The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.<br />  To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts<br />  the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the<br />  data in its raw, binary state. BigQuery also supports the escape sequence \"\\t\" to specify a tab separator.<br />  The default value is a comma (',').</p></li><li><p><code>skip_leading_rows</code> -<br />  (Optional)<br />  The number of rows at the top of a CSV file that BigQuery will skip when loading the data.<br />  The default value is 0. This property is useful if you have header rows in the file that should be skipped.<br />  When autodetect is on, the behavior is the following:<br />  skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,<br />  the row is read as data. Otherwise data is read starting from the second row.<br />  skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.<br />  skipLeadingRows = N &gt; 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,<br />  row N is just skipped. Otherwise row N is used to extract column names for the detected schema.</p></li><li><p><code>encoding</code> -<br />  (Optional)<br />  The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.<br />  The default value is UTF-8. BigQuery decodes the data after the raw, binary data<br />  has been split using the values of the quote and fieldDelimiter properties.</p></li><li><p><code>quote</code> -<br />  (Optional)<br />  The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,<br />  and then uses the first byte of the encoded string to split the data in its raw, binary state.<br />  The default value is a double-quote ('\"'). If your data does not contain quoted sections, set the property value to an empty string.<br />  If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.</p></li><li><p><code>max_bad_records</code> -<br />  (Optional)<br />  The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,<br />  an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.</p></li><li><p><code>allow_quoted_newlines</code> -<br />  (Optional)<br />  Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.<br />  The default value is false.</p></li><li><p><code>source_format</code> -<br />  (Optional)<br />  The format of the data files. For CSV files, specify \"CSV\". For datastore backups, specify \"DATASTORE_BACKUP\".<br />  For newline-delimited JSON, specify \"NEWLINE_DELIMITED_JSON\". For Avro, specify \"AVRO\". For parquet, specify \"PARQUET\".<br />  For orc, specify \"ORC\". [Beta] For Bigtable, specify \"BIGTABLE\".<br />  The default value is CSV.</p></li><li><p><code>json_extension</code> -<br />  (Optional)<br />  If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.<br />  For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited<br />  GeoJSON: set to GEOJSON.</p></li><li><p><code>allow_jagged_rows</code> -<br />  (Optional)<br />  Accept rows that are missing trailing optional columns. The missing values are treated as nulls.<br />  If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,<br />  an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.</p></li><li><p><code>ignore_unknown_values</code> -<br />  (Optional)<br />  Indicates if BigQuery should allow extra values that are not represented in the table schema.<br />  If true, the extra values are ignored. If false, records with extra columns are treated as bad records,<br />  and if there are too many bad records, an invalid error is returned in the job result.<br />  The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:<br />  CSV: Trailing columns<br />  JSON: Named values that don't match any column names</p></li><li><p><code>projection_fields</code> -<br />  (Optional)<br />  If sourceFormat is set to \"DATASTORE_BACKUP\", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.<br />  Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.<br />  If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.</p></li><li><p><code>autodetect</code> -<br />  (Optional)<br />  Indicates if we should automatically infer the options and schema for CSV and JSON sources.</p></li><li><p><code>schema_update_options</code> -<br />  (Optional)<br />  Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or<br />  supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;<br />  when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.<br />  For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:<br />  ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.<br />  ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.</p></li><li><p><code>time_partitioning</code> -<br />  (Optional)<br />  Time-based partitioning specification for the destination table.<br />  Structure is <a href=\"#nested_time_partitioning\">documented below</a>.</p></li><li><p><code>destination_encryption_configuration</code> -<br />  (Optional)<br />  Custom encryption configuration (e.g., Cloud KMS keys)<br />  Structure is <a href=\"#nested_destination_encryption_configuration\">documented below</a>.</p></li><li><p><code>parquet_options</code> -<br />  (Optional)<br />  Parquet Options for load and make external tables.<br />  Structure is <a href=\"#nested_parquet_options\">documented below</a>.</p></li></ul><p><a name=\"nested_destination_table\"></a>The <code>destination_table</code> block supports:</p><ul><li><p><code>project_id</code> -<br />  (Optional)<br />  The ID of the project containing this table.</p></li><li><p><code>dataset_id</code> -<br />  (Optional)<br />  The ID of the dataset containing this table.</p></li><li><p><code>table_id</code> -<br />  (Required)<br />  The table. Can be specified <code>{{table_id}}</code> if <code>project_id</code> and <code>dataset_id</code> are also set,<br />  or of the form <code>projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}</code> if not.</p></li></ul><p><a name=\"nested_time_partitioning\"></a>The <code>time_partitioning</code> block supports:</p><ul><li><p><code>type</code> -<br />  (Required)<br />  The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,<br />  but in OnePlatform the field will be treated as unset.</p></li><li><p><code>expiration_ms</code> -<br />  (Optional)<br />  Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.</p></li><li><p><code>field</code> -<br />  (Optional)<br />  If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.<br />  The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.<br />  A wrapper is used here because an empty string is an invalid value.</p></li></ul><p><a name=\"nested_destination_encryption_configuration\"></a>The <code>destination_encryption_configuration</code> block supports:</p><ul><li><p><code>kms_key_name</code> -<br />  (Required)<br />  Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.<br />  The BigQuery Service Account associated with your project requires access to this encryption key.</p></li><li><p><code>kms_key_version</code> -<br />  (Output)<br />  Describes the Cloud KMS encryption key version used to protect destination BigQuery table.</p></li></ul><p><a name=\"nested_parquet_options\"></a>The <code>parquet_options</code> block supports:</p><ul><li><p><code>enum_as_string</code> -<br />  (Optional)<br />  If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.</p></li><li><p><code>enable_list_inference</code> -<br />  (Optional)<br />  If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.</p></li></ul><p><a name=\"nested_copy\"></a>The <code>copy</code> block supports:</p><ul><li><p><code>source_tables</code> -<br />  (Required)<br />  Source tables to copy.<br />  Structure is <a href=\"#nested_source_tables\">documented below</a>.</p></li><li><p><code>destination_table</code> -<br />  (Optional)<br />  The destination table.<br />  Structure is <a href=\"#nested_destination_table\">documented below</a>.</p></li><li><p><code>create_disposition</code> -<br />  (Optional)<br />  Specifies whether the job is allowed to create new tables. The following values are supported:<br />  CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.<br />  CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.<br />  Creation, truncation and append actions occur as one atomic update upon job completion<br />  Default value is <code>CREATE_IF_NEEDED</code>.<br />  Possible values are: <code>CREATE_IF_NEEDED</code>, <code>CREATE_NEVER</code>.</p></li><li><p><code>write_disposition</code> -<br />  (Optional)<br />  Specifies the action that occurs if the destination table already exists. The following values are supported:<br />  WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.<br />  WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.<br />  WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.<br />  Each action is atomic and only occurs if BigQuery is able to complete the job successfully.<br />  Creation, truncation and append actions occur as one atomic update upon job completion.<br />  Default value is <code>WRITE_EMPTY</code>.<br />  Possible values are: <code>WRITE_TRUNCATE</code>, <code>WRITE_APPEND</code>, <code>WRITE_EMPTY</code>.</p></li><li><p><code>destination_encryption_configuration</code> -<br />  (Optional)<br />  Custom encryption configuration (e.g., Cloud KMS keys)<br />  Structure is <a href=\"#nested_destination_encryption_configuration\">documented below</a>.</p></li></ul><p><a name=\"nested_source_tables\"></a>The <code>source_tables</code> block supports:</p><ul><li><p><code>project_id</code> -<br />  (Optional)<br />  The ID of the project containing this table.</p></li><li><p><code>dataset_id</code> -<br />  (Optional)<br />  The ID of the dataset containing this table.</p></li><li><p><code>table_id</code> -<br />  (Required)<br />  The table. Can be specified <code>{{table_id}}</code> if <code>project_id</code> and <code>dataset_id</code> are also set,<br />  or of the form <code>projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}</code> if not.</p></li></ul><p><a name=\"nested_destination_table\"></a>The <code>destination_table</code> block supports:</p><ul><li><p><code>project_id</code> -<br />  (Optional)<br />  The ID of the project containing this table.</p></li><li><p><code>dataset_id</code> -<br />  (Optional)<br />  The ID of the dataset containing this table.</p></li><li><p><code>table_id</code> -<br />  (Required)<br />  The table. Can be specified <code>{{table_id}}</code> if <code>project_id</code> and <code>dataset_id</code> are also set,<br />  or of the form <code>projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}</code> if not.</p></li></ul><p><a name=\"nested_destination_encryption_configuration\"></a>The <code>destination_encryption_configuration</code> block supports:</p><ul><li><p><code>kms_key_name</code> -<br />  (Required)<br />  Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.<br />  The BigQuery Service Account associated with your project requires access to this encryption key.</p></li><li><p><code>kms_key_version</code> -<br />  (Output)<br />  Describes the Cloud KMS encryption key version used to protect destination BigQuery table.</p></li></ul><p><a name=\"nested_extract\"></a>The <code>extract</code> block supports:</p><ul><li><p><code>destination_uris</code> -<br />  (Required)<br />  A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.</p></li><li><p><code>print_header</code> -<br />  (Optional)<br />  Whether to print out a header row in the results. Default is true.</p></li><li><p><code>field_delimiter</code> -<br />  (Optional)<br />  When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.<br />  Default is ','</p></li><li><p><code>destination_format</code> -<br />  (Optional)<br />  The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.<br />  The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.<br />  The default value for models is SAVED_MODEL.</p></li><li><p><code>compression</code> -<br />  (Optional)<br />  The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.<br />  The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.</p></li><li><p><code>use_avro_logical_types</code> -<br />  (Optional)<br />  Whether to use logical types when extracting to AVRO format.</p></li><li><p><code>source_table</code> -<br />  (Optional)<br />  A reference to the table being exported.<br />  Structure is <a href=\"#nested_source_table\">documented below</a>.</p></li><li><p><code>source_model</code> -<br />  (Optional)<br />  A reference to the model being exported.<br />  Structure is <a href=\"#nested_source_model\">documented below</a>.</p></li></ul><p><a name=\"nested_source_table\"></a>The <code>source_table</code> block supports:</p><ul><li><p><code>project_id</code> -<br />  (Optional)<br />  The ID of the project containing this table.</p></li><li><p><code>dataset_id</code> -<br />  (Optional)<br />  The ID of the dataset containing this table.</p></li><li><p><code>table_id</code> -<br />  (Required)<br />  The table. Can be specified <code>{{table_id}}</code> if <code>project_id</code> and <code>dataset_id</code> are also set,<br />  or of the form <code>projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}</code> if not.</p></li></ul><p><a name=\"nested_source_model\"></a>The <code>source_model</code> block supports:</p><ul><li><p><code>project_id</code> -<br />  (Required)<br />  The ID of the project containing this model.</p></li><li><p><code>dataset_id</code> -<br />  (Required)<br />  The ID of the dataset containing this model.</p></li><li><p><code>model_id</code> -<br />  (Required)<br />  The ID of the model.</p></li></ul><hr /><ul><li><p><code>job_timeout_ms</code> -<br />  (Optional)<br />  Job timeout in milliseconds. If this time limit is exceeded, BigQuery may attempt to terminate the job.</p></li><li><p><code>labels</code> -<br />  (Optional)<br />  The labels associated with this job. You can use these to organize and group your jobs.</p></li><li><p><code>query</code> -<br />  (Optional)<br />  Configures a query job.<br />  Structure is <a href=\"#nested_query\">documented below</a>.</p></li><li><p><code>load</code> -<br />  (Optional)<br />  Configures a load job.<br />  Structure is <a href=\"#nested_load\">documented below</a>.</p></li><li><p><code>copy</code> -<br />  (Optional)<br />  Copies a table.<br />  Structure is <a href=\"#nested_copy\">documented below</a>.</p></li><li><p><code>extract</code> -<br />  (Optional)<br />  Configures an extract job.<br />  Structure is <a href=\"#nested_extract\">documented below</a>.</p></li><li><p><code>location</code> -<br />  (Optional)<br />  The geographic location of the job. The default value is US.</p></li><li><p><code>project</code> - (Optional) The ID of the project in which the resource belongs.<br />    If it is not provided, the provider project is used.</p></li></ul>", "attributes-reference": "<h2 id=\"attributes-reference\">Attributes Reference</h2><p>In addition to the arguments listed above, the following computed attributes are exported:</p><ul><li><p><code>id</code> - an identifier for the resource with format <code>projects/{{project}}/jobs/{{job_id}}</code></p></li><li><p><code>user_email</code> -<br />  Email address of the user who ran the job.</p></li><li><p><code>job_type</code> -<br />  (Output)<br />  The type of the job.</p></li><li><p><code>status</code> -<br />  The status of this job. Examine this value when polling an asynchronous job to see if the job is complete.<br />  Structure is <a href=\"#nested_status\">documented below</a>.</p></li></ul><p><a name=\"nested_status\"></a>The <code>status</code> block contains:</p><ul><li><p><code>error_result</code> -<br />  (Output)<br />  Final error result of the job. If present, indicates that the job has completed and was unsuccessful.<br />  Structure is <a href=\"#nested_error_result\">documented below</a>.</p></li><li><p><code>errors</code> -<br />  (Output)<br />  The first errors encountered during the running of the job. The final message<br />  includes the number of errors that caused the process to stop. Errors here do<br />  not necessarily mean that the job has not completed or was unsuccessful.<br />  Structure is <a href=\"#nested_errors\">documented below</a>.</p></li><li><p><code>state</code> -<br />  (Output)<br />  Running state of the job. Valid states include 'PENDING', 'RUNNING', and 'DONE'.</p></li></ul><p><a name=\"nested_error_result\"></a>The <code>error_result</code> block contains:</p><ul><li><p><code>reason</code> -<br />  (Optional)<br />  A short error code that summarizes the error.</p></li><li><p><code>location</code> -<br />  (Optional)<br />  Specifies where the error occurred, if present.</p></li><li><p><code>message</code> -<br />  (Optional)<br />  A human-readable description of the error.</p></li></ul><p><a name=\"nested_errors\"></a>The <code>errors</code> block contains:</p><ul><li><p><code>reason</code> -<br />  (Optional)<br />  A short error code that summarizes the error.</p></li><li><p><code>location</code> -<br />  (Optional)<br />  Specifies where the error occurred, if present.</p></li><li><p><code>message</code> -<br />  (Optional)<br />  A human-readable description of the error.</p></li></ul>", "timeouts": "<h2 id=\"timeouts\">Timeouts</h2><p>This resource provides the following<br /><a href=\"https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts\">Timeouts</a> configuration options:</p><ul><li><code>create</code> - Default is 20 minutes.</li><li><code>delete</code> - Default is 20 minutes.</li></ul>", "import": "<h2 id=\"import\">Import</h2><p>Job can be imported using any of these accepted formats:</p><p><code>$ terraform import google_bigquery_job.default projects/{{project}}/jobs/{{job_id}}/location/{{location}}$ terraform import google_bigquery_job.default projects/{{project}}/jobs/{{job_id}}$ terraform import google_bigquery_job.default {{project}}/{{job_id}}/{{location}}$ terraform import google_bigquery_job.default {{job_id}}/{{location}}$ terraform import google_bigquery_job.default {{project}}/{{job_id}}$ terraform import google_bigquery_job.default {{job_id}}</code></p>", "user-project-overrides": "<h2 id=\"user-project-overrides\">User Project Overrides</h2><p>This resource supports <a href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#user_project_override\">User Project Overrides</a>.</p>", "description": "<h1 id=\"google_bigquery_job\">google_bigquery_job</h1><p>Jobs are actions that BigQuery runs on your behalf to load data, export data, query data, or copy data.<br />Once a BigQuery job is created, it cannot be changed or deleted.</p><p>To get more information about Job, see:</p><ul><li><a href=\"https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs\">API documentation</a></li><li>How-to Guides<ul><li><a href=\"https://cloud.google.com/bigquery/docs/jobs-overview\">BigQuery Jobs Intro</a></li></ul></li></ul><div class = \"oics-button\" style=\"float: right; margin: 0 0 -15px\">  <a href=\"https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=bigquery_job_query&cloudshell_image=gcr.io%2Fcloudshell-images%2Fcloudshell%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md\" target=\"_blank\">    <img alt=\"Open in Cloud Shell\" src=\"//gstatic.com/cloudssh/images/open-btn.svg\" style=\"max-height: 44px; margin: 32px auto; max-width: 100%;\">  </a></div>"}