{"google-dataflow-flex-template-job": "<h1 id=\"google_dataflow_flex_template_job\">google_dataflow_flex_template_job</h1><p>Creates a <a href=\"https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates\">Flex Template</a><br />job on Dataflow, which is an implementation of Apache Beam running on Google<br />Compute Engine. For more information see the official documentation for <a href=\"https://beam.apache.org\">Beam</a><br />and <a href=\"https://cloud.google.com/dataflow/\">Dataflow</a>.</p><p>~&gt; <strong>Warning:</strong> This resource is in beta, and should be used with the terraform-provider-google-beta provider.<br />See <a href=\"https://terraform.io/docs/providers/google/guides/provider_versions.html\">Provider Versions</a> for more details on beta resources.</p>", "example-usage": "<h2 id=\"example-usage\">Example Usage</h2><p><code>hclresource \"google_dataflow_flex_template_job\" \"big_data_job\" {  provider                = google-beta  name                    = \"dataflow-flextemplates-job\"  container_spec_gcs_path = \"gs://my-bucket/templates/template.json\"  parameters = {    inputSubscription = \"messages\"  }}</code></p>", "note-on-destroy-apply": "<h2 id=\"note-on-destroy-apply\">Note on \"destroy\" / \"apply\"</h2><p>There are many types of Dataflow jobs.  Some Dataflow jobs run constantly,<br />getting new data from (e.g.) a GCS bucket, and outputting data continuously.<br />Some jobs process a set amount of data then terminate. All jobs can fail while<br />running due to programming errors or other issues. In this way, Dataflow jobs<br />are different from most other Terraform / Google resources.</p><p>The Dataflow resource is considered 'existing' while it is in a nonterminal<br />state.  If it reaches a terminal state (e.g. 'FAILED', 'COMPLETE',<br />'CANCELLED'), it will be recreated on the next 'apply'.  This is as expected for<br />jobs which run continuously, but may surprise users who use this resource for<br />other kinds of Dataflow jobs.</p><p>A Dataflow job which is 'destroyed' may be \"cancelled\" or \"drained\".  If<br />\"cancelled\", the job terminates - any data written remains where it is, but no<br />new data will be processed.  If \"drained\", no new data will enter the pipeline,<br />but any data currently in the pipeline will finish being processed.  The default<br />is \"cancelled\", but if a user sets <code>on_delete</code> to <code>\"drain\"</code> in the<br />configuration, you may experience a long wait for your <code>terraform destroy</code> to<br />complete.</p><p>You can potentially short-circuit the wait by setting <code>skip_wait_on_job_termination</code><br />to <code>true</code>, but beware that unless you take active steps to ensure that the job<br /><code>name</code> parameter changes between instances, the name will conflict and the launch<br />of the new job will fail. One way to do this is with a<br /><a href=\"https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/id\">random_id</a><br />resource, for example:</p><p><br />variable \"big_data_job_subscription_id\" {<br />  type    = string<br />  default = \"projects/myproject/subscriptions/messages\"<br />}</p><p>resource \"random_id\" \"big_data_job_name_suffix\" {<br />  byte_length = 4<br />  keepers = {<br />    region          = var.region<br />    subscription_id = var.big_data_job_subscription_id<br />  }<br />}<br />resource \"google_dataflow_flex_template_job\" \"big_data_job\" {<br />  provider                      = google-beta<br />  name                          = \"dataflow-flextemplates-job-${random_id.big_data_job_name_suffix.dec}\"<br />  region                        = var.region<br />  container_spec_gcs_path       = \"gs://my-bucket/templates/template.json\"<br />  skip_wait_on_job_termination = true<br />  parameters = {<br />    inputSubscription = var.big_data_job_subscription_id<br />  }<br />}<br /></p>", "argument-reference": "<h2 id=\"argument-reference\">Argument Reference</h2><p>The following arguments are supported:</p><ul><li><p><code>name</code> - (Required) A unique name for the resource, required by Dataflow.</p></li><li><p><code>container_spec_gcs_path</code> - (Required) The GCS path to the Dataflow job Flex<br />Template.</p></li></ul><hr /><ul><li><p><code>parameters</code> - (Optional) Key/Value pairs to be passed to the Dataflow job (as<br />used in the template). Additional <a href=\"https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options\">pipeline options</a><br />such as <code>serviceAccount</code>, <code>workerMachineType</code>, etc can be specified here.</p></li><li><p><code>labels</code> - (Optional) User labels to be specified for the job. Keys and values<br />should follow the restrictions specified in the <a href=\"https://cloud.google.com/compute/docs/labeling-resources#restrictions\">labeling restrictions</a><br />page. <br /><strong>NOTE</strong>: Google-provided Dataflow templates often provide default labels<br />that begin with <code>goog-dataflow-provided</code>. Unless explicitly set in config, these<br />labels will be ignored to prevent diffs on re-apply.</p></li><li><p><code>on_delete</code> - (Optional) One of \"drain\" or \"cancel\". Specifies behavior of<br />deletion during <code>terraform destroy</code>.  See above note.</p></li><li><p><code>skip_wait_on_job_termination</code> - (Optional)  If set to <code>true</code>, terraform will<br />treat <code>DRAINING</code> and <code>CANCELLING</code> as terminal states when deleting the resource,<br />and will remove the resource from terraform state and move on.  See above note.</p></li><li><p><code>project</code> - (Optional) The project in which the resource belongs. If it is not<br />provided, the provider project is used.</p></li><li><p><code>region</code> - (Optional) The region in which the created job should run.</p></li></ul>", "attributes-reference": "<h2 id=\"attributes-reference\">Attributes Reference</h2><p>In addition to the arguments listed above, the following computed attributes are exported:</p><ul><li><p><code>job_id</code> - The unique ID of this job.</p></li><li><p><code>state</code> - The current state of the resource, selected from the <a href=\"https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState\">JobState enum</a></p></li></ul>", "import": "<h2 id=\"import\">Import</h2><p>This resource does not support import.</p>", "description": "<h1 id=\"google_dataflow_flex_template_job\">google_dataflow_flex_template_job</h1><p>Creates a <a href=\"https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates\">Flex Template</a><br />job on Dataflow, which is an implementation of Apache Beam running on Google<br />Compute Engine. For more information see the official documentation for <a href=\"https://beam.apache.org\">Beam</a><br />and <a href=\"https://cloud.google.com/dataflow/\">Dataflow</a>.</p><p>~&gt; <strong>Warning:</strong> This resource is in beta, and should be used with the terraform-provider-google-beta provider.<br />See <a href=\"https://terraform.io/docs/providers/google/guides/provider_versions.html\">Provider Versions</a> for more details on beta resources.</p>"}